## IntroducciÃ³n
El desarrollo de agentes inteligentes para juegos estratÃ©gicos ha sido un tema central en el campo de la inteligencia artificial. Este trabajo se centra en el uso del aprendizaje por refuerzo (Reinforcement Learning, RL), una rama de aprendizaje automÃ¡tico en la que un agente interactÃºa con un entorno para maximizar una recompensa acumulada.

Quoridor se presenta como un desafÃ­o estratÃ©gico de gran interÃ©s en el Ã¡mbito de los juegos de tablero, al combinar la planificaciÃ³n a largo plazo con la adaptaciÃ³n a las acciones del oponente. En este proyecto, el aprendizaje por refuerzo se utiliza para explorar cÃ³mo un agente puede aprender a navegar un espacio de estados dinÃ¡mico, empleando representaciones eficientes del tablero, estrategias de movimiento y colocaciÃ³n de barreras.

Este trabajo se centra en el diseÃ±o, implementaciÃ³n y anÃ¡lisis de dos tipos de agentes para el juego Quoridor: un agente determinista, basado en reglas predefinidas, y un agente adaptativo, entrenado mediante Q-learning, un algoritmo de aprendizaje por refuerzo.

Para evaluar de manera efectiva al agente basado en Q-learning, se introdujeron algunas modificaciones al juego original. Entre ellas, un sistema de puntuaciÃ³n y un lÃ­mite de turnos, que permiten medir el desempeÃ±o en escenarios mÃ¡s controlados y facilitan el entrenamiento del agente.

El agente adaptativo serÃ¡ evaluado en partidas automÃ¡ticas contra un agente determinista, cuyo comportamiento estÃ¡ basado en un conjunto fijo de reglas, como avanzar prioritariamente hacia la meta y bloquear estratÃ©gicamente al oponente mediante la colocaciÃ³n de barreras. Esta comparaciÃ³n permitirÃ¡ analizar las diferencias en desempeÃ±o entre un enfoque basado en aprendizaje por refuerzo y otro basado en reglas fijas, destacando las ventajas de un agente que aprende y se adapta dinÃ¡micamente.

Este informe describe el desarrollo del proyecto en cinco secciones principales: el marco teÃ³rico, que aborda los fundamentos conceptuales del aprendizaje por refuerzo y su aplicaciÃ³n en Quoridor; el diseÃ±o experimental, donde se detalla la implementaciÃ³n de los agentes y las mÃ©tricas de evaluaciÃ³n; el anÃ¡lisis y discusiÃ³n de resultados, que interpreta los hallazgos obtenidos durante las pruebas; y las conclusiones, que sintetizan los aprendizajes y plantean posibles lÃ­neas futuras de trabajo.

## Marco TeÃ³rico 

Quoridor es un juego de tablero que plantea desafÃ­os estratÃ©gicos y tÃ¡cticos. Los jugadores deben avanzar hacia su objetivo mientras colocan barreras para dificultar el progreso del oponente, garantizando siempre un camino accesible hacia la meta. Esta combinaciÃ³n de planificaciÃ³n ofensiva y defensiva convierte a Quoridor en un entorno ideal para estudiar agentes inteligentes que toman decisiones en tiempo real.

#### Aprendizaje por Refuerzo
El aprendizaje por refuerzo se basa en la interacciÃ³n entre un agente y un entorno. El agente observa el estado del entorno, ejecuta una acciÃ³n, recibe una recompensa y transita hacia un nuevo estado. El objetivo del agente es aprender una polÃ­tica que maximice la recompensa acumulada a lo largo del tiempo. La polÃ­tica puede ser una funciÃ³n determinista o probabilÃ­stica que asocia estados con acciones.

Un concepto central en RL es la funciÃ³n de valor, que estima la recompensa esperada para un estado o una combinaciÃ³n estado-acciÃ³n. En este trabajo, se utiliza el algoritmo Q-learning, que aproxima la funciÃ³n de valor Ã³ptima (ğ‘„*) sin requerir un modelo explÃ­cito del entorno. Este algoritmo utiliza una estructura conocida como Q-table para almacenar los valores Q y actualizarlos mediante la fÃ³rmula:

    ğ‘„(ğ‘ ,ğ‘) â† ğ‘„(ğ‘ ,ğ‘) + ğ›¼ [ğ‘Ÿ + ğ›¾max~ğ‘â€²~ ğ‘„(ğ‘ â€²,ğ‘â€²) âˆ’ ğ‘„(ğ‘ ,ğ‘)]

Donde:

* ğ‘  y ğ‘ â€² son el estado actual y el siguiente.

* ğ‘ y ğ‘â€² son las acciones actuales y futuras.

* ğ‘Ÿ es la recompensa recibida.

* ğ›¼ es la tasa de aprendizaje.

* ğ›¾ es el factor de descuento.

#### AplicaciÃ³n en Quoridor

En el juego de Quoridor, el entorno estÃ¡ representado por un tablero de 9x9 casillas. En este proyecto, se trabajan dos variantes del juego: un tablero estÃ¡ndar de 9x9 casillas y una versiÃ³n simplificada de 5x5. El uso de un tablero mÃ¡s pequeÃ±o facilita la exploraciÃ³n y entrenamiento inicial del agente basado en Q-learning, permitiendo un anÃ¡lisis comparativo del impacto de la complejidad del entorno en el desempeÃ±o de los agentes.

AdemÃ¡s, se introducen modificaciones como un sistema de puntuaciÃ³n basado en el progreso hacia la meta, la colocaciÃ³n de barreras y la cantidad de turnos disponibles. Estas reglas adaptadas permiten evaluar de forma mÃ¡s granular las estrategias y decisiones tomadas por los agentes.

En este contexto los estados encapsulan informaciÃ³n sobre las posiciones de los peones, las barreras colocadas y las barreras restantes de cada jugador. Las acciones disponibles incluyen mover el peÃ³n a una casilla vÃ¡lida (horizontal, vertical o mediante saltos) y colocar barreras en ubicaciones permitidas. El objetivo del agente es llegar a la fila opuesta o maximizar su puntaje siguiendo el esquema adaptado.

La recompensa se diseÃ±a para reflejar el progreso estratÃ©gico:

    1- Movimientos hacia la meta: Otorgan recompensas crecientes (2^ğ‘›^, segÃºn la fila alcanzada).

    2- Movimientos en sentido opuesto: Se penalizan con 2^ğ‘›^, siguiendo el mismo criterio.

    3- ColocaciÃ³n de barreras: Reciben una recompensa fija positiva para incentivar el uso estratÃ©gico de este recurso.

Este diseÃ±o recompensa las acciones que acercan al agente a su objetivo, penaliza movimientos regresivos y fomenta el uso eficiente de las barreras.

#### El Agente Determinista

En contraste con el agente basado en Q-learning, el agente determinista sigue un conjunto de reglas predefinidas para tomar decisiones. Estas reglas incluyen avanzar hacia la meta siempre que sea posible, bloquear estratÃ©gicamente al oponente mediante la colocaciÃ³n de barreras en forma de "U", y realizar movimientos laterales o retrocesos cuando las opciones anteriores no estÃ©n disponibles. Este enfoque ofrece un comportamiento predecible y sirve como referencia para evaluar el desempeÃ±o del agente adaptativo.

#### ComparaciÃ³n de Enfoques

El enfrentamiento entre el agente determinista y el agente basado en Q-learning proporciona una base para analizar la eficacia de los enfoques basados en reglas frente a los adaptativos. Mientras que el agente determinista sigue estrategias fijas, el agente de Q-learning tiene la capacidad de ajustar su comportamiento a medida que aprende del entorno, lo que puede resultar en un desempeÃ±o superior en situaciones complejas o dinÃ¡micas.

#### ComparaciÃ³n con Otros MÃ©todos

Aunque este trabajo emplea Q-learning, otros enfoques, como el **Monte Carlo Tree Search (MCTS)**, han sido explorados previamente en Quoridor debido a su capacidad para evaluar de forma eficiente mÃºltiples secuencias de movimientos. Sin embargo, el MCTS es computacionalmente intensivo y requiere un modelo explÃ­cito del juego, lo que lo hace menos adecuado para entrenar agentes adaptativos en un entorno dinÃ¡mico. En contraste, Q-learning permite a los agentes aprender directamente de la interacciÃ³n con el entorno, adaptÃ¡ndose al comportamiento del oponente y a reglas modificadas.

#### Relevancia del Aprendizaje por Refuerzo en Quoridor

La naturaleza secuencial y competitiva de Quoridor lo convierte en un entorno ideal para investigar el balance entre exploraciÃ³n y explotaciÃ³n, el diseÃ±o de recompensas, y la representaciÃ³n eficiente de estados y acciones. AdemÃ¡s, las adaptaciones introducidas en este proyecto, como el sistema de puntuaciÃ³n y los tableros de diferentes tamaÃ±os, permiten evaluar cÃ³mo la complejidad del entorno afecta el aprendizaje y la estrategia del agente.

## BibliografÃ­a

[1] Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Pearson.

[2] Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.

[3] Lapan, M. (2018). Deep Reinforcement Learning Hands-On. Packt Publishing.

[4] MassaguÃ© Respall, V., Brown, J., & Aslam, H. (2018). Monte Carlo Tree Search for Quoridor. ResearchGate.

[5] Wang, H., Emmerich, M., & Plaat, A. (2019). Assessing the Potential of Classical Q-learning in General Game Playing. In Proceedings of the International Conference on Agents and Artificial Intelligence. ResearchGate.