## IntroducciÃ³n
El juego Quoridor se presenta como un desafÃ­o estratÃ©gico de gran interÃ©s en el campo de los juegos de tablero, combinando la planificaciÃ³n a largo plazo con la adaptaciÃ³n a las acciones del oponente. Este proyecto tiene como objetivo desarrollar un agente inteligente capaz de competir de manera eficaz en este entorno, empleando tÃ©cnicas de aprendizaje por refuerzo.

El aprendizaje por refuerzo ha demostrado ser una herramienta poderosa para diseÃ±ar sistemas autÃ³nomos que interactÃºan con entornos complejos, aprendiendo estrategias Ã³ptimas mediante prueba y error. Su implementaciÃ³n en Quoridor permite explorar cÃ³mo un agente puede aprender a navegar un espacio de estados dinÃ¡mico, utilizando representaciones eficientes del tablero, estrategias de movimiento y colocaciÃ³n de barreras.

Dado que el diseÃ±o y entrenamiento de agentes inteligentes en juegos como Quoridor plantean desafÃ­os significativos, este proyecto se centra en investigar el desempeÃ±o del algoritmo Q-learning, un enfoque clÃ¡sico de aprendizaje por refuerzo. El agente serÃ¡ evaluado en tableros de diferentes tamaÃ±os (5x5 y 9x9), adaptando las reglas y objetivos del juego para maximizar la eficiencia del entrenamiento y analizar su impacto en la estrategia del agente.

AdemÃ¡s, se introducen algunas modificaciones, como un sistema de puntuaciÃ³n y un lÃ­mite de turnos, para explorar variantes del juego que ofrezcan un balance entre simplicidad y complejidad estratÃ©gica.

Este informe describe el desarrollo del proyecto en cinco secciones principales: el marco teÃ³rico, que aborda los fundamentos conceptuales del aprendizaje por refuerzo y su aplicaciÃ³n en Quoridor; el diseÃ±o experimental, donde se detalla la implementaciÃ³n del agente y las mÃ©tricas de evaluaciÃ³n; el anÃ¡lisis y discusiÃ³n de resultados, que interpreta los hallazgos obtenidos durante las pruebas; y las conclusiones, que sintetizan los aprendizajes y plantean posibles lÃ­neas futuras de trabajo.

## Marco TeÃ³rico

El desarrollo de agentes inteligentes para juegos estratÃ©gicos ha sido un tema central en el campo de la inteligencia artificial. Este trabajo se centra en el uso del aprendizaje por refuerzo (Reinforcement Learning, RL), una rama de aprendizaje automÃ¡tico en la que un agente interactÃºa con un entorno para maximizar una recompensa acumulada. En este contexto, el juego Quoridor se utiliza como caso de estudio, siendo un entorno ideal para investigar la toma de decisiones secuenciales, la planificaciÃ³n estratÃ©gica y la interacciÃ³n competitiva.

#### Aprendizaje por Refuerzo
El aprendizaje por refuerzo se basa en la interacciÃ³n entre un agente y un entorno. El agente observa el estado del entorno, ejecuta una acciÃ³n, recibe una recompensa y transita hacia un nuevo estado. El objetivo del agente es aprender una polÃ­tica que maximice la recompensa acumulada a lo largo del tiempo. La polÃ­tica puede ser una funciÃ³n determinista o probabilÃ­stica que asocia estados con acciones.

Un concepto central en RL es la funciÃ³n de valor, que estima la recompensa esperada para un estado o una combinaciÃ³n estado-acciÃ³n. En este trabajo, se utiliza el algoritmo Q-learning, que aproxima la funciÃ³n de valor Ã³ptima (ğ‘„*) sin requerir un modelo explÃ­cito del entorno. Este algoritmo utiliza una estructura conocida como Q-table para almacenar los valores Q y actualizarlos mediante la fÃ³rmula:

    ğ‘„(ğ‘ ,ğ‘) â† ğ‘„(ğ‘ ,ğ‘) + ğ›¼ [ğ‘Ÿ + ğ›¾max~ğ‘â€²~ ğ‘„(ğ‘ â€²,ğ‘â€²) âˆ’ ğ‘„(ğ‘ ,ğ‘)]

Donde:

* ğ‘  y ğ‘ â€² son el estado actual y el siguiente.

* ğ‘ y ğ‘â€² son las acciones actuales y futuras.

* ğ‘Ÿ es la recompensa recibida.

* ğ›¼ es la tasa de aprendizaje.

* ğ›¾ es el factor de descuento.

#### AplicaciÃ³n en Quoridor

En el juego de Quoridor, el entorno estÃ¡ representado por un tablero de 9x9 o 5x5 casillas. Los estados encapsulan informaciÃ³n sobre las posiciones de los peones, las barreras colocadas y las barreras restantes de cada jugador. Las acciones disponibles incluyen mover el peÃ³n a una casilla vÃ¡lida (horizontal, vertical o mediante saltos) y colocar barreras en ubicaciones permitidas. El objetivo del agente es llegar a la fila opuesta o maximizar su puntaje siguiendo el esquema adaptado.

La recompensa se diseÃ±a para reflejar el progreso estratÃ©gico:

    1- Movimientos hacia la meta: Otorgan recompensas crecientes (2^ğ‘›^, segÃºn la fila alcanzada).

    2- Movimientos en sentido opuesto: Se penalizan con 2^ğ‘›^, siguiendo el mismo criterio.

    3- ColocaciÃ³n de barreras: Reciben una recompensa fija positiva para incentivar el uso estratÃ©gico de este recurso.

Este diseÃ±o recompensa las acciones que acercan al agente a su objetivo, penaliza movimientos regresivos y fomenta el uso eficiente de las barreras.

#### ComparaciÃ³n con Otros MÃ©todos

Aunque este trabajo emplea Q-learning, otros enfoques, como el **Monte Carlo Tree Search (MCTS)**, han sido explorados previamente en Quoridor debido a su capacidad para evaluar de forma eficiente mÃºltiples secuencias de movimientos. Sin embargo, el MCTS es computacionalmente intensivo y requiere un modelo explÃ­cito del juego, lo que lo hace menos adecuado para entrenar agentes adaptativos en un entorno dinÃ¡mico. En contraste, Q-learning permite a los agentes aprender directamente de la interacciÃ³n con el entorno, adaptÃ¡ndose al comportamiento del oponente y a reglas modificadas.

#### Relevancia del Aprendizaje por Refuerzo en Quoridor

La naturaleza secuencial y competitiva de Quoridor lo convierte en un entorno ideal para investigar el balance entre exploraciÃ³n y explotaciÃ³n, el diseÃ±o de recompensas, y la representaciÃ³n eficiente de estados y acciones. AdemÃ¡s, las adaptaciones introducidas en este proyecto, como el sistema de puntuaciÃ³n y los tableros de diferentes tamaÃ±os, permiten evaluar cÃ³mo la complejidad del entorno afecta el aprendizaje y la estrategia del agente.

## BibliografÃ­a

[1] Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Pearson.

[2] Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.

[3] Lapan, M. (2018). Deep Reinforcement Learning Hands-On. Packt Publishing.

[4] MassaguÃ© Respall, V., Brown, J., & Aslam, H. (2018). Monte Carlo Tree Search for Quoridor. ResearchGate.

[5] Wang, H., Emmerich, M., & Plaat, A. (2019). Assessing the Potential of Classical Q-learning in General Game Playing. In Proceedings of the International Conference on Agents and Artificial Intelligence. ResearchGate.