# Notas
El objetivo de estas notas es apuntar toda la informaci칩n que pueda ser 칰til tanto para el desarrollo del proyecto como para la elaboraci칩n del informe.

## Conceptos claves de cada libro y articulo seleccionados

### [1] Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach

#### Conceptos clave:
* Introducci칩n al Reinforcement Learning como un enfoque central para sistemas inteligentes aut칩nomos.
* Explicaci칩n de c칩mo los estados, acciones, y recompensas definen el marco del aprendizaje por refuerzo.
* Revisi칩n de modelos de decisi칩n y c칩mo estos influyen en el dise침o de pol칤ticas.

#### Relevancia para el proyecto:
* Sirve como base conceptual para entender el marco general de los agentes inteligentes que pueden aplicarse en el dise침o del agente de Quoridor.
* Proporciona herramientas te칩ricas para analizar decisiones y estrategias 칩ptimas en un entorno competitivo como Quoridor.

### [2] Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction

#### Conceptos clave:
* Profundizaci칩n en el aprendizaje basado en pruebas y errores, con 칠nfasis en el equilibrio entre exploraci칩n y explotaci칩n.
* Desglose del Q-learning como un algoritmo clave de aprendizaje por refuerzo.
* Discusi칩n sobre el dise침o de la funci칩n de recompensa y su impacto en el aprendizaje del agente.
* Explicaci칩n de c칩mo se modelan las pol칤ticas y su relaci칩n con los valores de estado y acci칩n.

#### Relevancia para el proyecto:
* Sirve como una gu칤a detallada para implementar Q-learning en el agente para Quoridor.
* Ayuda a estructurar las recompensas y pol칤ticas necesarias para entrenar un agente capaz de tomar decisiones estrat칠gicas en el juego.
* Permite entender c칩mo manejar las transiciones entre estados de un tablero din치mico como el de Quoridor.

### [3] Lapan, M. (2018). Deep Reinforcement Learning Hands-On

#### Conceptos clave:

* Integraci칩n de Deep Learning con algoritmos de aprendizaje por refuerzo, como Q-learning y variantes como DQN.
* Aplicaciones pr치cticas del aprendizaje por refuerzo en entornos complejos.


#### Relevancia para el proyecto:
* Ofrece ejemplos aplicados que pueden inspirar estrategias avanzadas para el agente de Quoridor, como combinar redes neuronales con Q-learning para explorar estados complejos.
* Es 칰til si es necesario ampliar el enfoque m치s all치 del cl치sico Q-learning hacia m칠todos m치s robustos para manejar m칰ltiples reglas o modificaciones.

### [4] Massagu칠 Respall, V., Brown, J., & Aslam, H. (2018). Monte Carlo Tree Search for Quoridor

#### Conceptos clave:
* Estudio detallado de la representaci칩n de estados y acciones en Quoridor.
* An치lisis de c칩mo los m칠todos basados en Monte Carlo Tree Search (MCTS) pueden usarse para optimizar decisiones en este juego.

#### Relevancia para el proyecto:
* Proporciona una base para representar el tablero de Quoridor, las posiciones de los jugadores y las barreras como estados para el agente.
* Aunque se centra en MCTS, los conceptos de representaci칩n de estados y acciones son directamente aplicables para Q-learning.
* Permite comparar Q-learning con enfoques determin칤sticos como MCTS en t칠rminos de efectividad y complejidad computacional.

### [5] Wang, H., Emmerich, M., & Plaat, A. (2019). Assessing the Potential of Classical Q-learning in General Game Playing

#### Conceptos clave:
* Evaluaci칩n del desempe침o de Q-learning en juegos generales, con 칠nfasis en c칩mo adaptar el algoritmo a diferentes escenarios.
* Discusi칩n sobre el impacto del dise침o de la funci칩n de recompensa y la representaci칩n de las pol칤ticas en el 칠xito del agente.

#### Relevancia para el proyecto:
* Brinda ejemplos concretos de implementaci칩n de Q-learning en juegos, que pueden adaptarse a Quoridor.
* Discute las limitaciones del algoritmo en t칠rminos de tiempo de convergencia y la necesidad de dise침ar recompensas efectivas, aspectos clave para el proyecto.
* Ayuda a entender c칩mo modelar estrategias para entornos como Quoridor, donde las reglas y los objetivos son espec칤ficos.

## Glosario de conceptos

### Aprendizaje por Refuerzo (Reinforcement Learning, RL)
El aprendizaje por refuerzo es un paradigma del aprendizaje autom치tico donde un agente aprende a interactuar con un entorno tomando decisiones secuenciales para maximizar una recompensa acumulada a lo largo del tiempo. Basado en prueba y error, el agente debe equilibrar la exploraci칩n de nuevas acciones con la explotaci칩n de estrategias ya conocidas. En el contexto de Quoridor, este enfoque permite desarrollar un agente que aprenda a moverse estrat칠gicamente en el tablero, adapt치ndose a la din치mica del juego y a las decisiones del oponente.

### Estado (State)
Un estado representa una configuraci칩n del entorno en un momento dado, descrita por un conjunto de caracter칤sticas relevantes para el agente. En Quoridor, el estado se define por la posici칩n de los peones en el tablero, la distribuci칩n de las barreras y los movimientos disponibles. Capturar esta informaci칩n en un formato adecuado es esencial para que el agente pueda evaluar sus opciones y tomar decisiones informadas.

### Acci칩n (Action)
Una acci칩n es la decisi칩n que el agente toma en un estado particular para interactuar con el entorno, lo que genera una transici칩n hacia un nuevo estado. En Quoridor, las acciones incluyen mover el pe칩n a una casilla adyacente o colocar una barrera en una posici칩n v치lida. Estas decisiones impactan directamente en el progreso hacia la meta del agente y en las posibilidades del oponente, lo que las convierte en un componente cr칤tico del aprendizaje.

### Pol칤tica (Policy)
La pol칤tica es la estrategia que gu칤a al agente en la selecci칩n de acciones seg칰n el estado en el que se encuentra. Puede ser determin칤stica, indicando una 칰nica acci칩n por estado, o estoc치stica, asignando probabilidades a diferentes acciones. En Quoridor, dise침ar una pol칤tica 칩ptima implica priorizar movimientos que acerquen al agente a su objetivo mientras se obstaculiza el progreso del adversario, maximizando las posibilidades de ganar.

### Funci칩n de Recompensa (Reward Function)
La funci칩n de recompensa asigna un valor num칠rico al agente como retroalimentaci칩n por realizar una acci칩n en un estado espec칤fico. En Quoridor, esta funci칩n puede dise침arse para otorgar recompensas positivas por movimientos que acerquen al pe칩n del agente a la meta, o penalizaciones por acciones que no contribuyan al progreso, como colocar barreras que dificulten su propio camino. Este dise침o es crucial para guiar el aprendizaje hacia estrategias efectivas.

### Q-learning
El Q-learning es un algoritmo de aprendizaje por refuerzo que actualiza valores de calidad (Q-values) para cada combinaci칩n de estado y acci칩n, aproximando una pol칤tica 칩ptima sin requerir un modelo expl칤cito del entorno. En Quoridor, el Q-learning permite al agente evaluar las posibles decisiones en cada turno, identificando movimientos que maximicen las recompensas acumuladas. Su capacidad para adaptarse a las transiciones din치micas del tablero lo hace especialmente adecuado para este juego.

### Tabla Q (Q-table)
La Q-table es una estructura de datos utilizada en Q-learning para almacenar los valores Q asociados a cada par estado-acci칩n. En Quoridor, esta tabla es fundamental para evaluar qu칠 acciones son m치s ventajosas en un estado dado. Sin embargo, dado el gran espacio de estados posibles en el tablero, podr칤a ser necesario utilizar t칠cnicas de aproximaci칩n para manejar la complejidad.

### Exploraci칩n vs Explotaci칩n
El dilema de exploraci칩n y explotaci칩n equilibra la necesidad de probar nuevas acciones para descubrir mejores recompensas y de aprovechar acciones previamente identificadas como beneficiosas. En Quoridor, un agente que explore demasiado podr칤a perder oportunidades claras de progreso, mientras que uno que solo explote estrategias conocidas podr칤a quedar atrapado en soluciones sub칩ptimas. Un buen balance entre ambos es clave para desarrollar estrategias adaptativas y efectivas.

### Factor de Descuento (洧)
El factor de descuento controla cu치nto valora el agente las recompensas futuras en comparaci칩n con las inmediatas.

### Funci칩n de Valor (Value Function)
La funci칩n de valor estima la recompensa acumulada esperada para un estado o un par estado-acci칩n bajo una pol칤tica espec칤fica. En el contexto de Quoridor, esta funci칩n ayuda al agente a identificar estados ventajosos y a priorizar movimientos que maximicen su progreso hacia la meta.

### Entorno (Environment)
El entorno es el sistema con el que interact칰a el agente, respondiendo a sus acciones con nuevos estados y recompensas. En Quoridor, el entorno se define por el tablero, las posiciones de los jugadores, las barreras y las reglas del juego. Dise침ar este entorno de manera precisa es esencial para que el agente comprenda y act칰e de manera efectiva en el juego.

## Quoridor

### Presentaci칩n
* Un tablero de 9x9 casillas.
* 20 barreras y 2 peones.

### Finalidad del juego

Llegar el primero a la l칤nea opuesta de su 
salida (fig. 7).

### REGLAS PARA 2 JUGADORES
* Antes de empezar, a cada jugador se le otorgan 10 barreras.
* Cada jugador coloca su pe칩n en el centro de su l칤nea de salida (fig. 1).
* Sortear para saber quien empieza.

### Desarrollo de una partida

    1- Cuando sea su turno, el jugador puede elegir desplazar su pe칩n o colocar una barrera.
    
    2- Si ya ha desplazado todas sus barreras, el jugador solamente podr치 desplazar su pe칩n.

    3- Movimiento de los peones: Los peones pueden desplazarse de una casilla y solo de una en una, en sentido horizontal o vertical, avanzando o retrocediendo, (fig. 2). Hay que evitar las barreras (fig.3).

    4- Disposici칩n de las barreras: Las barreras se disponen exactamente entre 2 bloques de 2 casillas (fig 4).

    5- Las barreras se utilizar치n para que el adversario pueda avanzar menos, no obstante esta prohibido impedirle completamente el acceso hacia su l칤nea de llegada: Siempre hay que dejar un acceso libre (fig 5).

    6- Frente a frente: Cuando los 2 peones se encuentran cara a cara sobre 2 casillas vecinas sin que una barrera los separe, el jugador que le toque jugar puede saltar y colocarse delante del pe칩n del jugador adversario (fig 6,8 ,9).

### Fin de la partida
El primer jugador que llegue a la novena casilla, enfrente de su l칤nea de salida, gana la partida.

### Adaptaci칩n para el proyecto

Si bien el juego permite jugar de hasta 4 jugadores, cada uno con el objetivo de llegar al lado opuesto de donde comienza, s칩lo se tomar치 el juego para 2 jugadores.

Como se utilizar치 tableros de 5x5 adem치s del original de 9x9, la cantidad de barreras en ese caso estar치 limitada a 3. 

Adem치s, el **fin de la partida no ocurrir치 cuando un pe칩n llegue al lado contrario(cada vez que llegue a la meta volver치 a la posic칩n original), sino que se contar치 con una cantidad limitada de turnos y ganar치 el jugador que logre la mayor cantidad de puntos**.

#### Sistema de puntaje

    1- Por cada casillero vertical que se avance hacia la meta sumara 2^n puntos, donde n indica la fila desde el punto de partida.
    2- Cada barrera colocada sumara 10 puntos.
    3- Los movimientos laterales no sumaran, ni restar치n puntos.
    4- Los movimientos en sentido a la fila de inicio restar치n 2^n puntos, de la misma manera que se indica en el punto 1.

### Justificaci칩n del uso del tablero de 5x5

#### Simplicidad y Rapidez en Entrenamiento:
* Menor Complejidad: Un tablero m치s peque침o tiene un espacio de estados reducido, lo que facilita la exploraci칩n completa del espacio de estados y acciones.
* R치pido Entrenamiento: El agente puede aprender m치s r치pidamente en un entorno m치s simple, permitiendo iteraciones m치s r치pidas y pruebas de conceptos.

#### Pruebas Iniciales:

* Validaci칩n de Conceptos: Usar un tablero m치s peque침o para probar y validar algoritmos b치sicos antes de aplicarlos a un entorno m치s complejo.
* Depuraci칩n y ajustes: Facilita la identificaci칩n y correcci칩n de errores en la implementaci칩n del algoritmo y la estructura del entorno.

#### Recursos Computacionales:

* Menos demanda de memoria y procesamiento: El entrenamiento en un tablero m치s peque침o requiere menos memoria y capacidad de procesamiento, lo que es beneficioso si los recursos son limitados.