# Desempe√±o y B√∫squeda de Pol√≠ticas de Agente Reinforcement Learning en Quoridor(DYBPARLQ)

## Introducci√≥n
El desarrollo de agentes inteligentes para juegos estrat√©gicos ha sido un tema central en el campo de la inteligencia artificial. Este trabajo se centra en el uso del aprendizaje por refuerzo (Reinforcement Learning, RL), una rama de aprendizaje autom√°tico en la que un agente interact√∫a con un entorno para maximizar una recompensa acumulada.

Quoridor se presenta como un desaf√≠o estrat√©gico de gran inter√©s en el √°mbito de los juegos de tablero, al combinar la planificaci√≥n a largo plazo con la adaptaci√≥n a las acciones del oponente. En este proyecto, el aprendizaje por refuerzo se utiliza para explorar c√≥mo un agente puede aprender a navegar un espacio de estados din√°mico, empleando representaciones eficientes del tablero, estrategias de movimiento y colocaci√≥n de barreras.

Este trabajo se centra en el dise√±o, implementaci√≥n y an√°lisis de dos tipos de agentes para el juego Quoridor: un agente determinista, basado en reglas predefinidas, y un agente adaptativo, entrenado mediante Q-learning, un algoritmo de aprendizaje por refuerzo.

Para evaluar de manera efectiva al agente basado en Q-learning, se introdujeron algunas modificaciones al juego original. Entre ellas, un sistema de puntuaci√≥n y un l√≠mite de turnos, que permiten medir el desempe√±o en escenarios m√°s controlados y facilitan el entrenamiento del agente.

El agente adaptativo ser√° evaluado en partidas autom√°ticas contra un agente determinista, cuyo comportamiento est√° basado en un conjunto fijo de reglas, como avanzar prioritariamente hacia la meta y bloquear estrat√©gicamente al oponente mediante la colocaci√≥n de barreras. Esta comparaci√≥n permitir√° analizar las diferencias en desempe√±o entre un enfoque basado en aprendizaje por refuerzo y otro basado en reglas fijas, destacando las ventajas de un agente que aprende y se adapta din√°micamente.

Este informe describe el desarrollo del proyecto en cinco secciones principales: el marco te√≥rico, que aborda los fundamentos conceptuales del aprendizaje por refuerzo y su aplicaci√≥n en Quoridor; el dise√±o experimental, donde se detalla la implementaci√≥n de los agentes y las m√©tricas de evaluaci√≥n; el an√°lisis y discusi√≥n de resultados, que interpreta los hallazgos obtenidos durante las pruebas; y las conclusiones, que sintetizan los aprendizajes y plantean posibles l√≠neas futuras de trabajo.

## Marco Te√≥rico 

Quoridor es un juego de tablero que plantea desaf√≠os estrat√©gicos y t√°cticos. Los jugadores deben avanzar hacia su objetivo mientras colocan barreras para dificultar el progreso del oponente, garantizando siempre un camino accesible hacia la meta. Esta combinaci√≥n de planificaci√≥n ofensiva y defensiva convierte a Quoridor en un entorno ideal para estudiar agentes inteligentes que toman decisiones en tiempo real.

### Reglas de Quoridor

Quoridor es un juego de mesa en el que el objetivo original es llegar a la fila opuesta del tablero antes que tu oponente, evitando que el contrincante llegue a su objetivo colocando obst√°culos. El juego est√° compuesto por el tablero que est√° formado por una cuadr√≠cula de 9x9 casillas, 2 peones(uno para cada jugador) y 20 muros o barreras(10 para cada jugador).

#### Preparaci√≥n:
Cada jugador coloca su pe√≥n en la casilla central de su fila inicial (la fila m√°s cercana a √©l). Cada jugador recibe 10 muros.

#### Turnos:
Los jugadores se turnan para jugar. En su turno, un jugador puede hacer una de las siguientes acciones:

1. Mover su pe√≥n: El pe√≥n puede moverse a una casilla adyacente en direcci√≥n ortogonal (arriba, abajo, izquierda o derecha), siempre que no haya un muro bloqueando el paso.
2. Colocar un muro: Se puede colocar un muro entre dos casillas para bloquear el paso del oponente. Los muros deben colocarse entre dos intersecciones de l√≠neas y no pueden dividir el tablero en dos partes completamente aisladas.

#### Reglas adicionales:

* Si un pe√≥n est√° adyacente al pe√≥n del oponente y no hay un muro entre ellos, puede saltarlo y ocupar su casilla.
* Si hay un muro detr√°s del pe√≥n del oponente, se puede mover en direcci√≥n lateral.
* No se pueden recuperar muros una vez colocados.

El juego termina cuando un jugador alcanza la fila opuesta con su pe√≥n y se le declara ganador.

### Aprendizaje por Refuerzo
El aprendizaje por refuerzo se basa en la interacci√≥n entre un agente y un entorno. El agente observa el estado del entorno, ejecuta una acci√≥n, recibe una recompensa y transita hacia un nuevo estado. El objetivo del agente es aprender una pol√≠tica que maximice la recompensa acumulada a lo largo del tiempo. La pol√≠tica puede ser una funci√≥n determinista o probabil√≠stica que asocia estados con acciones.

Un concepto central en RL es la funci√≥n de valor, que estima la recompensa esperada para un estado o una combinaci√≥n estado-acci√≥n. En este trabajo, se utiliza el algoritmo Q-learning, que aproxima la funci√≥n de valor √≥ptima (ùëÑ*) sin requerir un modelo expl√≠cito del entorno. Este algoritmo utiliza una estructura conocida como Q-table para almacenar los valores Q y actualizarlos mediante la f√≥rmula:

    ùëÑ(ùë†,ùëé) ‚Üê ùëÑ(ùë†,ùëé) + ùõº [ùëü + ùõæmax~ùëé‚Ä≤~ ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤) ‚àí ùëÑ(ùë†,ùëé)]

Donde:

* ùë† y ùë†‚Ä≤ son el estado actual y el siguiente.

* ùëé y ùëé‚Ä≤ son las acciones actuales y futuras.

* ùëü es la recompensa recibida.

* ùõº es la tasa de aprendizaje.

* ùõæ es el factor de descuento.

### Importancia del Aprendizaje por Refuerzo en Juegos de Mesa

El aprendizaje por refuerzo (RL) ha sido fundamental en la resoluci√≥n de juegos de mesa cl√°sicos, permitiendo a las m√°quinas aprender estrategias complejas sin conocimiento previo expl√≠cito del dominio. Ejemplos destacados incluyen AlphaGo, que super√≥ a jugadores profesionales de Go utilizando una combinaci√≥n de RL y redes neuronales profundas, y AlphaZero, que demostr√≥ una capacidad sobresaliente en ajedrez, shogi(un juego japon√©s parecido al ajedrez) y GO, partiendo solo de reglas b√°sicas y autoaprendizaje.

La aplicaci√≥n de RL en Quoridor representa un desaf√≠o similar, ya que involucra la planificaci√≥n estrat√©gica y la adaptaci√≥n a las decisiones del oponente. A diferencia de juegos como el ajedrez, donde las piezas tienen reglas de movimiento fijas, Quoridor introduce barreras que pueden modificar el espacio de juego en cada turno, lo que aumenta la complejidad del aprendizaje.

### Aplicaci√≥n en Quoridor

En el juego de Quoridor, el entorno est√° representado por un tablero de 9x9 casillas. En este proyecto, se trabajan dos variantes del juego: un tablero est√°ndar de 9x9 casillas y una versi√≥n simplificada de 5x5. El uso de un tablero m√°s peque√±o facilita la exploraci√≥n y entrenamiento inicial del agente basado en Q-learning, permitiendo un an√°lisis comparativo del impacto de la complejidad del entorno en el desempe√±o de los agentes.

Adem√°s, se introducen modificaciones como un sistema de puntuaci√≥n basado en el progreso hacia la meta, la colocaci√≥n de barreras y la cantidad de turnos disponibles. Estas reglas adaptadas permiten evaluar de forma m√°s granular las estrategias y decisiones tomadas por los agentes.

En este contexto los estados encapsulan informaci√≥n sobre las posiciones de los peones, las barreras colocadas y las barreras restantes de cada jugador. Las acciones disponibles incluyen mover el pe√≥n a una casilla v√°lida (horizontal, vertical o mediante saltos) y colocar barreras en ubicaciones permitidas. El objetivo del agente es llegar a la fila opuesta o maximizar su puntaje siguiendo el esquema adaptado.

La recompensa se dise√±a para reflejar el progreso estrat√©gico:

    1- Movimientos hacia la meta: Otorgan recompensas crecientes (2^ùëõ, seg√∫n la fila alcanzada).

    2- Movimientos en sentido opuesto: Se penalizan con 2^ùëõ, siguiendo el mismo criterio.

    3- Colocaci√≥n de barreras: Reciben una recompensa fija positiva para incentivar el uso estrat√©gico de este recurso.

Este dise√±o recompensa las acciones que acercan al agente a su objetivo, penaliza movimientos regresivos y fomenta el uso eficiente de las barreras.

### El Agente Determinista

En contraste con el agente basado en Q-learning, el agente determinista sigue un conjunto de reglas predefinidas para tomar decisiones. Estas reglas incluyen avanzar hacia la meta siempre que sea posible, bloquear estrat√©gicamente al oponente mediante la colocaci√≥n de barreras en forma de "U", y realizar movimientos laterales o retrocesos cuando las opciones anteriores no est√©n disponibles. Este enfoque ofrece un comportamiento predecible y sirve como referencia para evaluar el desempe√±o del agente adaptativo.

### Comparaci√≥n de Enfoques

El enfrentamiento entre el agente determinista y el agente basado en Q-learning proporciona una base para analizar la eficacia de los enfoques basados en reglas frente a los adaptativos. Mientras que el agente determinista sigue estrategias fijas, el agente de Q-learning tiene la capacidad de ajustar su comportamiento a medida que aprende del entorno, lo que puede resultar en un desempe√±o superior en situaciones complejas o din√°micas.

#### Comparaci√≥n con Otros M√©todos

Aunque este trabajo emplea Q-learning, otros enfoques, como el **Monte Carlo Tree Search (MCTS)**, han sido explorados previamente en Quoridor debido a su capacidad para evaluar de forma eficiente m√∫ltiples secuencias de movimientos. Sin embargo, el MCTS es computacionalmente intensivo y requiere un modelo expl√≠cito del juego, lo que lo hace menos adecuado para entrenar agentes adaptativos en un entorno din√°mico. En contraste, Q-learning permite a los agentes aprender directamente de la interacci√≥n con el entorno, adapt√°ndose al comportamiento del oponente y a reglas modificadas.

En problemas de mayor complejidad, como juegos con grandes espacios de estado, se podr√≠an utilizar t√©cnicas m√°s avanzadas, como Deep Q-Networks (DQN). En el caso de Quoridor, el n√∫mero de estados posibles es extremadamente grande debido a la combinaci√≥n de posiciones de los jugadores y las barreras colocadas. Esto hace que una Q-table tradicional sea dif√≠cil de manejar, pues requiere almacenar una cantidad masiva de combinaciones de estados y acciones.Para abordar el problema del espacio de estado demasiado grande, Deep Q-Networks (DQN) reemplaza la Q-table con una red neuronal que aproxima la funci√≥n Q(s,a). En lugar de almacenar valores expl√≠citos en una tabla, la red neuronal aprende a predecir la mejor acci√≥n en funci√≥n de las caracter√≠sticas del estado.

Si bien en este trabajo se ha optado por Q-learning debido a su simplicidad y menor costo computacional, un enfoque basado en DQN podr√≠a ser m√°s adecuado para un agente que juegue Quoridor de manera √≥ptima. Esto se debe a que:

* La representaci√≥n expl√≠cita de todos los estados en una tabla es impracticable en Quoridor debido al gran espacio de b√∫squeda.
* Una red neuronal podr√≠a aprender patrones generales del juego sin necesidad de almacenar cada estado individualmente.
* DQN permitir√≠a la generalizaci√≥n en tableros de diferentes tama√±os o contra distintos tipos de oponentes.


### Relevancia del Aprendizaje por Refuerzo en Quoridor

La naturaleza secuencial y competitiva de Quoridor lo convierte en un entorno ideal para investigar el balance entre exploraci√≥n y explotaci√≥n, el dise√±o de recompensas, y la representaci√≥n eficiente de estados y acciones. Adem√°s, las adaptaciones introducidas en este proyecto, como el sistema de puntuaci√≥n y los tableros de diferentes tama√±os, permiten evaluar c√≥mo la complejidad del entorno afecta el aprendizaje y la estrategia del agente.

### Exploraci√≥n vs. Explotaci√≥n en el Aprendizaje por Refuerzo
Uno de los desaf√≠os fundamentales en el aprendizaje por refuerzo es el equilibrio entre exploraci√≥n y explotaci√≥n. Este dilema surge porque el agente debe decidir constantemente entre:

**Explotaci√≥n**: Elegir la mejor acci√≥n conocida hasta el momento para maximizar la recompensa inmediata.
**Exploraci√≥n**: Probar nuevas acciones para descubrir estrategias potencialmente mejores a largo plazo.

En el contexto de Quoridor, la explotaci√≥n significar√≠a que el agente selecciona siempre el movimiento con la mejor recompensa esperada seg√∫n su Q-table. Sin embargo, si el agente explota demasiado temprano, puede quedar atrapado en estrategias sub√≥ptimas sin descubrir mejores caminos para ganar la partida.

Por otro lado, la exploraci√≥n implica tomar decisiones que pueden parecer inicialmente malas (como colocar barreras en posiciones poco intuitivas) con la esperanza de descubrir patrones estrat√©gicos m√°s efectivos a largo plazo. Un agente que explora demasiado puede desperdiciar movimientos y aprender m√°s lentamente, pero tambi√©n puede encontrar soluciones m√°s √≥ptimas con el tiempo.

####  M√©todos de Exploraci√≥n en Q-learning
Para gestionar este equilibrio, existen diversas estrategias de exploraci√≥n:

**Œµ-greedy**: Se elige la mejor acci√≥n conocida con probabilidad 1‚àíŒµ, y una acci√≥n aleatoria con probabilidad Œµ disminuye progresivamente para explorar m√°s al inicio y explotar en etapas avanzadas.

**Softmax**: Las acciones se eligen de forma probabil√≠stica seg√∫n sus valores Q(s,a), permitiendo que incluso las opciones sub√≥ptimas sean seleccionadas ocasionalmente.

**M√©todos basados en incertidumbre (UCB, Bayesian Methods)**: Favorecen acciones menos exploradas, enfoc√°ndose en reducir la incertidumbre en lugar de maximizar recompensas inmediatas.

##### Justificaci√≥n del Uso de Œµ-greedy en Quoridor
La elecci√≥n de Œµ-greedy para este proyecto se basa en los siguientes factores:

1. Facilidad de implementaci√≥n: Es un enfoque simple y efectivo que no requiere c√°lculos adicionales de probabilidades o modelos de incertidumbre.
2. Adecuaci√≥n a un espacio de acciones discreto: En Quoridor, las acciones (moverse o colocar barreras) son discretas y finitas, lo que hace que este m√©todo sea eficiente.
3. Balance din√°mico entre exploraci√≥n y explotaci√≥n: Al decrecer Œµ con el tiempo, el agente puede explorar al inicio y explotar estrategias aprendidas en etapas posteriores del entrenamiento.
4. Evita el estancamiento en estrategias locales sub√≥ptimas: La introducci√≥n de exploraci√≥n aleatoria permite que el agente descubra mejores estrategias en tableros con configuraciones variadas.

Dado que el objetivo es que el agente aprenda estrategias ganadoras sin quedar atrapado en soluciones mediocres, Œµ-greedy proporciona una forma pr√°ctica de permitir exploraci√≥n en las primeras etapas del aprendizaje y favorecer la explotaci√≥n conforme el agente se vuelve m√°s experto en el juego.

### Desaf√≠os de Convergencia de Q-learning en Espacios de Estado Grandes

Uno de los principales desaf√≠os en el uso de Q-learning es garantizar su convergencia cuando se enfrenta a espacios de estados grandes y complejos, como el tablero de Quoridor. A medida que el n√∫mero de estados crece exponencialmente con el tama√±o del tablero y la cantidad de barreras colocadas, la tabla Q(s,a) se vuelve dif√≠cil de almacenar y actualizar eficientemente. Esto no solo ralentiza el aprendizaje, sino que tambi√©n incrementa el riesgo de que el agente no logre explorar suficientemente el espacio de soluciones, afectando su capacidad para encontrar estrategias √≥ptimas, por lo que nos encontramos con los siguientes desaf√≠os:

1. Dimensionalidad del Espacio de Estados

En Quoridor, un estado est√° definido por:

* La posici√≥n de ambos jugadores.
* La cantidad y ubicaci√≥n de las barreras en el tablero.

Esto genera un n√∫mero exponencialmente creciente de posibles configuraciones. En problemas con espacios de estado enormes, almacenar y actualizar una tabla Q(s,a) expl√≠cita se vuelve inviable tanto en t√©rminos de memoria como de tiempo de c√≥mputo.

2. Convergencia Lenta

El aprendizaje en Q-learning requiere visitar m√∫ltiples veces cada estado y acci√≥n para estimar correctamente sus valores Q(s,a). Sin embargo, en espacios de estado grandes:

* Muchos estados pueden no visitarse con suficiente frecuencia, retrasando la convergencia.
* Peque√±os errores en la actualizaci√≥n de Q pueden propagarse y afectar decisiones futuras.
* La exploraci√≥n puede ser ineficiente si no se aplican estrategias bien dise√±adas, como epsilon decay.

3. Problema de Generalizaci√≥n

Q-learning est√°ndar no generaliza entre estados similares. Cada estado se trata de manera independiente, lo que significa que incluso si dos estados son casi id√©nticos, sus valores  Q(s,a) se aprenden por separado. Por ejemplo, si el agente ha aprendido que bloquear el camino del oponente en una posici√≥n espec√≠fica es √∫til, este conocimiento no se transfiere autom√°ticamente a otras configuraciones similares del tablero.

En este proyecto, Q-learning ha sido implementado de manera tradicional debido a su simplicidad, pero los desaf√≠os mencionados han limitado su capacidad para encontrar estrategias √≥ptimas en todos los escenarios.

## Dise√±o Experimental

Para poner a prueba el algoritmo de Q-learning aplicado al entorno de quoridor se tiene que tener en cuenta por un lado la manera en la que se va a recompensar o penalizar al agente por cada acci√≥n que tome seg√∫n el estado en el que se encuentre, y por el otro lado hacer un proceso se sintonizaci√≥n o tuning de los hiper-par√°metros del algoritmo, es decir, se van a ajustar los distintos par√°metros del algoritmo de q-learning(alpha, gamma, epsilon), como as√≠ tambi√©n considerar que factor de descuento de la tasa de exploraci√≥n(epsilon decay) tras cada episodio para buscar el mejor rendimiento en el problema de Quoridor, ya que al haber un espacio de acciones tan grande, si bien hay que fomentar la exploraci√≥n del agente, a medida que avanza el aprendizaje hay que disminuir paulatinamente este par√°metro para explotar las mejores acciones conocidas. Otro factor que hay que tener en cuenta es el entorno en el que se va a realizar esta sintonizaci√≥n, si bien el tablero original en el que se eval√∫a el agente es de 9x9, se pueden reducir los tiempos de este proceso llev√°ndolo a cabo en el entorno de 5x5 debido al gran tiempo de c√≥mputo que requieren las simulaciones.

Una vez obtenidos los datos de las simulaciones con las distintas combinaciones de par√°metros, se proceder√° a utilizar librer√≠as de python para graficar los resultados y as√≠ poder visualizar de mejor manera la forma en la que var√≠a el desempe√±o del agente con las variantes probadas. Este proceso se repetir√° las veces que sea necesario para poder ajustar lo mejor posible la configuraci√≥n del agente.

### Elecci√≥n de los Hiperpar√°metros de Q-learning

El entrenamiento requiere ajustes en:

* Tasa de aprendizaje (alpha): Determina cu√°nta informaci√≥n nueva se incorpora en cada iteraci√≥n.

* Factor de descuento (gamma): Controla cu√°nta importancia se da a las recompensas futuras.

* Epsilon (exploraci√≥n vs. explotaci√≥n): Define la probabilidad de elegir una acci√≥n aleatoria en lugar de la mejor conocida.

* Epsilon decay: Reduce gradualmente el epsilon a medida que el agente aprende.

### Primer Enfoque

En un primer enfoque para encontrar los valores √≥ptimos a utilizar, se busc√≥ en trabajos realizados utilizando Q-learning aplicado a juegos(Gymnasium, DataCamp) para ver c√≥mo se desempe√±a el agente en el entorno de Quoridor con esta configuraci√≥n. Se prob√≥ en ambos escenarios, tanto en el tablero de 5x5 y 9x9 para ver si hab√≠a diferencias significativas, donde se pudo apreciar r√°pidamente que en el entorno reducido se obten√≠an mejores resultados, y luego de var√≠as simulaciones, se pudo deducir que esto ocurr√≠a porque el espacio de acciones en el entorno original era mucho mayor al entorno reducido, ya que contaba con m√°s barreras para utilizar y m√°s casilleros para posicionarlas.

En este primer enfoque se realizaron pruebas individuales, que no han sido las m√°s adecuadas, ya que se podr√≠a hacer un an√°lisis m√°s completo y optimizar tiempos haciendo una barrido autom√°tico que abarque m√°s variaciones de los par√°metros. Si bien estos valores utilizados en las pruebas manuales no ten√≠an un desempe√±o tan malo, este enfoque no pod√≠a asegurar un rendimiento √≥ptimo, los valores que se utilizaron para los par√°metros estaban probados en otro tipo de entorno, con otras reglas y un an√°lisis de recompensas distinto al que se le estaba dando a el agente de Quoridor, por lo que se va armar un programa que pruebe m√°s configuraciones para el agente de manera autom√°tica y as√≠ lograr un an√°lisis m√°s exhaustivo del rendimiento del algoritmo en los distintos escenarios.

### Segundo Enfoque

En este segundo enfoque se busca hacer un barrido m√°s exhaustivo de los par√°metros mencionados, buscando encontrar el rango de valores donde el agente tiene los mejores rendimientos para luego poder refinarlos, a continuaci√≥n se detallan los valores que tendr√°n los mismos:

**alpha**: [0.01, 0.1, 0.5, 0.9]
**gamma**: [0.01, 0.1, 0.5, 0.9]
**epsilon**: [0.01, 0.1, 0.5, 0.9]
**epsilon-decay**: [0.99, 0.995]

y se tomaron distintos valores para los episodios a realizar en cada entrenamiento [500, 1000, 10000] para tener una primera aproximaci√≥n del comportamiento del agente con estos par√°metros, y ya que existe probabilidad de exploraci√≥n aleatoria, con una misma configuraci√≥n pueden haber resultados variables, es por eso que para reducir la variabilidad y mejorar la fiabilidad de los resultados se va a correr cada configuraci√≥n 5 veces, aunque para tener resultados verdaderamente fiables requerir√≠a de m√°s iteraciones, en este primer experimento no se van a realizar m√°s de 5 simulaciones por configuraci√≥n debido al tiempo de c√≥mputo de las mismas, luego en el entorno original de 9x9 ya teniendo los par√°metros que funcionaron mejor en estas pruebas, se podr√°n hacer m√°s simulaciones con las configuraciones con mejor rendimiento para obtener resultados m√°s fiables.

### Tercer Enfoque

Debido a que la cantidad de combinaciones a probar en el segundo enfoque era de 384, y la simulaci√≥n de cada una se iba a correr 5 veces, el total de simulaciones escalaba a 1920, adem√°s en las que la cantidad de episodios de entrenamiento eran de 10000 daba como resultado un tiempo considerablemente grande para los experimentos. Por lo dicho anteriormente se lleg√≥ a un tercer enfoque en el que los valores elegidos para correr los experimentos eran menos y m√°s cerca de donde se hab√≠an dado los mejores resultados en los enfoques anteriores. Los siguientes valores a probar son:

**alpha**: [0.1, 0.2, 0.5]
**gamma**: [0.5, 0.9]
**epsilon**: [0.5, 0.9]
**epsilon-decay**: [0.99, 0.995]

y adem√°s, ya que las pruebas se har√°n en el entorno reducido donde el espacio de acciones es menor, se redujo la cantidad m√°xima de episodios a 2000.

### Consideraciones para el Entorno de 9x9
Dado que en el entorno de 5x5 el agente pudo alcanzar buenos puntajes con relativamente pocos episodios y una tasa de exploraci√≥n moderada, es probable que en el entorno original (9x9) se requiera:

* Un Gamma m√°s alto (0.9 o superior) para priorizar estrategias a largo plazo.
* Un Epsilon Decay m√°s lento para permitir exploraci√≥n en un espacio de estados m√°s grande.
* Un n√∫mero mayor de episodios para permitir la convergencia de la pol√≠tica de juego.

### Evaluaci√≥n del Agente en el Entorno de 9x9

Tras analizar los resultados obtenidos en el entorno reducido de 5x5, se decidi√≥ evaluar el desempe√±o del agente en el entorno original de Quoridor (9x9) utilizando las configuraciones m√°s prometedoras. Dado que la expansi√≥n del espacio de estados y de acciones introduce una mayor complejidad en el aprendizaje, se ajustaron los hiperpar√°metros para favorecer una exploraci√≥n inicial m√°s amplia y permitir una adaptaci√≥n progresiva.

Los valores de los hiperpar√°metros seleccionados para esta evaluaci√≥n son los siguientes:

* Alpha: Se probar√°n los valores 0.1 y 0.5, ya que en las simulaciones previas se observ√≥ que valores m√°s altos de alpha aceleraban el aprendizaje en etapas iniciales, mientras que valores bajos permit√≠an mayor estabilidad en la convergencia.  
* Gamma: Se establece en 0.9 debido a su capacidad de equilibrar la importancia de recompensas futuras y evitar un comportamiento excesivamente miope. En el entorno 9x9, la planificaci√≥n a largo plazo es a√∫n m√°s crucial debido a la mayor cantidad de movimientos posibles antes de alcanzar una victoria o derrota.  
* Epsilon: Se fija en 1.0, lo que implica una exploraci√≥n completa en las primeras etapas del entrenamiento. Esto es fundamental para permitir al agente descubrir estrategias viables en el entorno m√°s amplio.  
* Epsilon Decay (tasa de disminuci√≥n de epsilon): Se establece en 0.99 para garantizar una transici√≥n gradual entre la exploraci√≥n y la explotaci√≥n, evitando que el agente se quede atrapado en estrategias sub√≥ptimas demasiado pronto.  
* Episodes: Se consideran valores de 50, 100, 250, 500, 750 y 1000 episodios para analizar c√≥mo evoluciona el aprendizaje en funci√≥n del tiempo de entrenamiento.


Esta configuraci√≥n permitir√° evaluar c√≥mo el agente transfiere y adapta su aprendizaje al entorno original, comparando su desempe√±o con los resultados obtenidos en 5x5. Adem√°s, se analizar√° c√≥mo var√≠a la recompensa a lo largo de los episodios y si la exploraci√≥n inicial contribuye a una mejor convergencia en este espacio de estados m√°s complejo. Y seg√∫n el tiempo de c√≥mputo de los escenarios con mayor cantidad de episodios, se podr√≠a probar aumentar el n√∫mero de episodios y establecer un epsilon m√≠nimo, para evitar estancamientos en soluciones sub√≥ptimas.

## An√°lisis de Resultados
El siguiente an√°lisis tiene como objetivo evaluar el desempe√±o del agente Q-learning primeramente en el entorno de Quoridor de 5x5 bajo distintas configuraciones de hiperpar√°metros. Para ello, se han realizado simulaciones variando valores clave como la tasa de aprendizaje (Œ±), el factor de descuento (Œ≥), la exploraci√≥n inicial (Œµ), la tasa de decaimiento de Œµ y la cantidad de episodios de entrenamiento. Se han registrado los puntajes promedio obtenidos por el agente para cada combinaci√≥n de estos par√°metros con el fin de identificar aquellas configuraciones que favorecen un mejor desempe√±o en la tarea de toma de decisiones dentro del juego.

En primer lugar, se presentar√°n los resultados globales de las simulaciones, analizando tendencias generales y destacando las configuraciones que han demostrado un mejor rendimiento. Luego, se examinar√° la evoluci√≥n del puntaje a lo largo de los episodios de entrenamiento para comprender c√≥mo el agente mejora su estrategia con el tiempo. Finalmente, a partir de este an√°lisis, se seleccionar√°n las configuraciones m√°s prometedoras para su evaluaci√≥n en el entorno original de Quoridor (9x9), permitiendo as√≠ validar su efectividad en un espacio de estados m√°s complejo.

### An√°lisis de Resultados en el Entorno 5x5

Los resultados obtenidos en las simulaciones de Q-learning en el entorno reducido de Quoridor (5x5) muestran tendencias claras en el impacto de los hiperpar√°metros sobre el desempe√±o del agente. La mejor configuraci√≥n identificada, con un puntaje promedio de 19.2, utiliz√≥ un Alpha de 0.5, Gamma de 0.5, Epsilon de 0.5, Epsilon Decay de 0.995 y 1000 episodios. Sin embargo, estos valores no necesariamente representan la configuraci√≥n √≥ptima en t√©rminos generales, ya que cada hiperpar√°metro introduce compromisos entre exploraci√≥n y explotaci√≥n, aprendizaje a corto o largo plazo, y estabilidad en la convergencia.

| Alpha | Gamma | Epsilon | Epsilon\_Decay | Episodes | Simulations | Avg\_Score |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 0.5 | 0.5 | 0.5 | 0.995 | 1000 | 5 | 19.2 |
| 0.5 | 0.9 | 0.5 | 0.995 | 1000 | 5 | 17.6 |
| 0.2 | 0.5 | 0.5 | 0.995 | 1000 | 5 | 16.8 |
| 0.5 | 0.5 | 0.9 | 0.995 | 1000 | 5 | 16.6 |
| 0.2 | 0.5 | 0.9 | 0.995 | 1000 | 5 | 15.6 |

***Tabla 1: Mejores 5 resultados en el entorno reducido tras 5 iteraciones***

#### Impacto de los Hiperpar√°metros

##### Alpha (Tasa de aprendizaje)
Un valor alto de Alpha (0.5) indica que el agente prioriza las recompensas m√°s recientes sobre la experiencia pasada. Si bien esto puede acelerar el aprendizaje en entornos peque√±os como el 5x5, tambi√©n implica que el agente puede olvidar estrategias efectivas previamente aprendidas. Un valor menor de Alpha podr√≠a favorecer la estabilidad en entornos m√°s complejos, donde el conocimiento acumulado es m√°s relevante.

##### Gamma (Factor de descuento)
La mejor configuraci√≥n utiliz√≥ un Gamma de 0.5, lo que significa que el agente dio un peso similar a las recompensas inmediatas y futuras. En contraste, algunas configuraciones con Gamma 0.9 tambi√©n obtuvieron buenos resultados (por ejemplo, puntaje promedio de 17.6), lo que sugiere que considerar m√°s fuertemente las recompensas a largo plazo podr√≠a ser beneficioso si el n√∫mero de episodios es mayor. En entornos m√°s grandes como Quoridor 9x9, donde las partidas duran m√°s turnos, un mayor Gamma podr√≠a ser m√°s efectivo.

##### Epsilon y Epsilon Decay (Exploraci√≥n vs. Explotaci√≥n)

Un Epsilon de 0.5 combinado con un Epsilon Decay de 0.995 parece haber permitido un equilibrio entre exploraci√≥n y explotaci√≥n. Un Epsilon alto al inicio fomenta la exploraci√≥n de nuevas estrategias, mientras que el decaimiento gradual reduce la aleatoriedad con el tiempo para permitir que el agente explote las estrategias aprendidas. Sin embargo, en entornos m√°s grandes, una exploraci√≥n m√°s prolongada podr√≠a ser necesaria antes de que la explotaci√≥n sea efectiva.

##### N√∫mero de episodios
El n√∫mero de episodios influye directamente en la capacidad del agente para refinar su estrategia. Configuraciones con 2000 episodios generalmente obtuvieron mejores resultados que aquellas con menos episodios, aunque no siempre de forma lineal. Esto sugiere que, aunque el agente sigue mejorando con m√°s entrenamiento, su tasa de aprendizaje puede volverse marginal con el tiempo.

![progresion_score_cada_25_episodios_5x5](./code/results-images/training_progress_5x5.png)
***Imagen 1: Variaci√≥n del score cada 10 episodios en el entorno reducido - alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_decay_rate=0.995***

![progresion_score_cada_25_episodios_9x9](./code/results-images/training_progress.png)
***Imagen 2: Variaci√≥n del score cada 25 episodios en el entorno original - alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_decay_rate=0.995***

### An√°lisis de Resultados en el Entorno 9x9

Los resultados obtenidos en el entorno 9x9 presentan una tendencia menos clara en comparaci√≥n con el entorno 5x5, lo que era esperable debido al mayor tama√±o del espacio de estados. Se pueden destacar varios aspectos clave en el an√°lisis del desempe√±o del agente:


| Alpha | Gamma | Epsilon | Epsilon\_Decay | Episodes | Simulations | Avg\_Score |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 0.1 | 0.9 | 1.0 | 0.99 | 50 | 3 | \-530.66 |
| 0.1 | 0.9 | 1.0 | 0.99 | 100 | 3 | \-488.0 |
| 0.1 | 0.9 | 1.0 | 0.99 | 250 | 3 | \-845.33 |
| 0.1 | 0.9 | 1.0 | 0.99 | 500 | 3 | \-939.33 |
| 0.1 | 0.9 | 1.0 | 0.99 | 750 | 3 | \-760.0 |
| 0.1 | 0.9 | 1.0 | 0.99 | 1000 | 3 | \-502.0 |
| 0.5 | 0.9 | 1.0 | 0.99 | 50 | 3 | \-503.33 |
| 0.5 | 0.9 | 1.0 | 0.99 | 100 | 3 | \-778.66 |
| 0.5 | 0.9 | 1.0 | 0.99 | 250 | 3 | \-598.66 |
| 0.5 | 0.9 | 1.0 | 0.99 | 500 | 3 | \-450.66 |
| 0.5 | 0.9 | 1.0 | 0.99 | 750 | 3 | \-684.0 |
| 0.5 | 0.9 | 1.0 | 0.99 | 1000 | 3 | \-496.0 |

![avg_score_per_training_episodes_9x9](./code/results-images/avg_score_vs_episodes_9x9.png)

#### Impacto del N√∫mero de Episodios

No se observa una mejora consistente a medida que aumenta el n√∫mero de episodios. Por ejemplo, con Œ± \= 0.1, el peor rendimiento ocurre con 500 episodios (-939.33), pero mejora considerablemente en 1000 episodios (-502).  
Con Œ± \= 0.5, la mejor media de recompensa se alcanza con 500 episodios (-450.66), pero el rendimiento disminuye en 750 episodios (-684) y vuelve a mejorar en 1000 episodios (-496).  
Esto sugiere que, aunque un mayor n√∫mero de episodios permite m√°s aprendizaje, la interacci√≥n entre la tasa de aprendizaje y la exploraci√≥n puede generar fluctuaciones en la recompensa obtenida.

#### Efecto de la Tasa de Aprendizaje (Alpha)

Con Œ± \= 0.1, el aprendizaje es m√°s conservador, lo que podr√≠a explicar por qu√© algunas configuraciones presentan valores m√°s negativos en episodios intermedios.  
Con Œ± \= 0.5, el agente ajusta sus Q-values m√°s r√°pido, lo que en algunos casos genera mejores resultados (ej. 500 episodios), pero tambi√©n puede causar inestabilidad en el aprendizaje.  
Esto sugiere que un Œ± moderado podr√≠a ser m√°s adecuado para este entorno, evitando tanto la convergencia excesivamente lenta como las oscilaciones bruscas en la pol√≠tica aprendida.

#### Exploraci√≥n y Decaimiento de Epsilon

Se utiliz√≥ Œµ \= 1.0 con un decaimiento de 0.99, lo que implica que el agente inicia explorando completamente y va reduciendo la exploraci√≥n progresivamente.  
En un entorno grande como 9x9, el n√∫mero de episodios actuales podr√≠a no ser suficiente para que el agente comience a explotar lo aprendido, lo que explicar√≠a la alta variabilidad en los resultados.  
Para mitigar este problema, se implementar√° un l√≠mite inferior en epsilon para garantizar que el agente conserve cierta capacidad de exploraci√≥n en episodios avanzados y no quede atrapado en una pol√≠tica sub√≥ptima.

#### Limitaciones por la Cantidad de Simulaciones

Dado el tiempo que toma cada simulaci√≥n, solo se realizaron 3 simulaciones por configuraci√≥n, lo que limita la confiabilidad de los resultados obtenidos.  
Algunas fluctuaciones en los valores podr√≠an deberse a la variabilidad inherente del proceso de aprendizaje en lugar de diferencias significativas en el desempe√±o del agente.  
Para mejorar la validez estad√≠stica del an√°lisis, se deber√≠a incrementar la cantidad de simulaciones por configuraci√≥n en futuros experimentos.

#### Comparaci√≥n con el Entorno 5x5

En comparaci√≥n con el entorno m√°s peque√±o, el desempe√±o en 9x9 es considerablemente m√°s bajo, lo que refleja la mayor dificultad de aprender en un espacio de estados m√°s grande.  
La convergencia en este entorno es m√°s lenta y podr√≠a requerir m√°s episodios o ajustes en los hiperpar√°metros para mejorar la estabilidad del aprendizaje.

### Cota inferior en la tasa de aprendizaje

En esta fase del experimento, se realizaron pruebas en ambos entornos(5x5 y 9x9) con los siguientes parametros: alpha=0.1, gamma=0.90, epsilon=1.0, epsilon\_decay\_rate=0.995, episodes= 2000, utilizando un valor m√≠nimo de œµ=0.1. Esta restricci√≥n se implement√≥ con el objetivo de evitar la convergencia prematura a pol√≠ticas sub√≥ptimas y permitir que el agente contin√∫e explorando posibles mejores soluciones en etapas avanzadas del entrenamiento.

A lo largo del entrenamiento, œµ decreci√≥ progresivamente hasta alcanzar el valor m√≠nimo establecido, de esta manera se busc√≥ evitar que el agente quedara atrapado en "pozos" de soluciones sub√≥ptimas sin la posibilidad de mejorar su desempe√±o mediante exploraci√≥n adicional.

Con estos resultados, se proceder√° a comparar el rendimiento del agente dichos entornos, analizando las diferencias en aprendizaje, estabilidad y desempe√±o final.

![progresion_score_cada_10_episodios_5x5](./code/results-images/training_progress_5x5-2.png)
***Imagen 3: Variaci√≥n del score cada 10 episodios en el entorno reducido - alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay_rate=0.995***

![progresion_score_cada_10_episodios_9x9](./code/results-images/training_progress_9x9-2.png)
***Imagen 4: Variaci√≥n del score cada 10 episodios en el entorno original - alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay_rate=0.995***

#### Comparaci√≥n del Aprendizaje

Durante las primeras etapas del entrenamiento, en ambos entornos el agente experiment√≥ una alta variabilidad en los puntajes, lo que indica un periodo de exploraci√≥n activa. Sin embargo, se observan diferencias significativas en la rapidez de aprendizaje entre ambos escenarios:

En el entorno 5x5, los puntajes iniciales muestran fluctuaciones considerables, pero alrededor del episodio 200 se observa una mejora progresiva en el desempe√±o del agente.

En el entorno 9x9, el proceso de aprendizaje es mucho m√°s lento, con puntajes negativos elevados que persisten a lo largo de las primeras 300-400 iteraciones, lo que sugiere un entorno m√°s desafiante y con mayor riesgo de estancamiento en soluciones sub√≥ptimas.

#### Estabilidad del Agente
La estabilidad del aprendizaje se refleja en la consistencia de los puntajes obtenidos una vez alcanzado el l√≠mite m√≠nimo de Œµ (epsilon=0.1). En el entorno 5x5, se observa que el agente comienza a obtener puntajes positivos de manera recurrente despu√©s del episodio 300, mientras que en el entorno 9x9 los valores negativos contin√∫an predominando, aunque con menor magnitud conforme avanza el entrenamiento.

En el entorno 5x5, el agente consigue valores positivos m√°s frecuentemente despu√©s de la fase exploratoria, reflejando un aprendizaje estable.

En el entorno 9x9, a pesar de una mejora en la reducci√≥n de los puntajes negativos, la variabilidad sigue siendo alta, lo que indica una falta de estabilidad en la estrategia aprendida.

#### Desempe√±o Final

Al comparar los √∫ltimos episodios, se evidencia una clara diferencia en el desempe√±o final del agente en ambos entornos:

En 5x5, el agente logra puntajes positivos en una cantidad significativa de episodios, con valores como 72, 52 y 60 en diversas iteraciones finales, lo que sugiere que encontr√≥ una estrategia efectiva.

En 9x9, a pesar de la mejora progresiva, los puntajes siguen siendo predominantemente negativos, con valores entre \-400 y \-1000 en m√∫ltiples iteraciones, indicando que el agente a√∫n no ha logrado una estrategia completamente eficiente para este entorno m√°s complejo.

## Bibliograf√≠a

[1] Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Pearson.

[2] Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.

[3] Lapan, M. (2018). Deep Reinforcement Learning Hands-On. Packt Publishing.

[4] Massagu√© Respall, V., Brown, J., & Aslam, H. (2018). Monte Carlo Tree Search for Quoridor. ResearchGate.

[5] Wang, H., Emmerich, M., & Plaat, A. (2019). Assessing the Potential of Classical Q-learning in General Game Playing. In Proceedings of the International Conference on Agents and Artificial Intelligence. ResearchGate.

[6] DataCamp. (n.d.). Introduction to Q-Learning: Beginner Tutorial. Retrieved from [https://www.datacamp.com/es/tutorial/introduction-q-learning-beginner-tutorial](https://www.datacamp.com/es/tutorial/introduction-q-learning-beginner-tutorial)

[7] Gymnasium. (n.d.). Train an Agent using Gymnasium. Retrieved from https://gymnasium.farama.org/