# Desempe√±o y B√∫squeda de Pol√≠ticas de Agente Reinforcement Learning en Quoridor(DYBPARLQ)

## Introducci√≥n

El desarrollo de agentes inteligentes para juegos estrat√©gicos ha sido un tema central en el campo de la inteligencia artificial. Este trabajo se centra en el uso del aprendizaje por refuerzo (Reinforcement Learning, RL), una rama de aprendizaje autom√°tico en la que un agente interact√∫a con un entorno para maximizar una recompensa acumulada.

Quoridor se presenta como un desaf√≠o estrat√©gico de gran inter√©s en el √°mbito de los juegos de tablero, al combinar la planificaci√≥n a largo plazo con la adaptaci√≥n a las acciones del oponente. En este proyecto, el aprendizaje por refuerzo se utiliza para explorar c√≥mo un agente puede aprender a navegar un espacio de estados din√°mico, empleando representaciones eficientes del tablero, estrategias de movimiento y colocaci√≥n de barreras.

Este trabajo se centra en el dise√±o, implementaci√≥n y an√°lisis de dos tipos de agentes para el juego Quoridor: un agente determinista, basado en reglas predefinidas, y un agente adaptativo, entrenado mediante Q-learning, un algoritmo de aprendizaje por refuerzo.

El agente adaptativo ser√° evaluado en partidas autom√°ticas contra un agente determinista, cuyo comportamiento est√° basado en un conjunto fijo de reglas, como avanzar prioritariamente hacia la meta y bloquear estrat√©gicamente al oponente mediante la colocaci√≥n de barreras. Esta comparaci√≥n permitir√° analizar las diferencias en desempe√±o entre un enfoque basado en aprendizaje por refuerzo y otro basado en reglas fijas, destacando las ventajas de un agente que aprende y se adapta din√°micamente.

Este informe describe el desarrollo del proyecto en cinco secciones principales: el marco te√≥rico, que aborda los fundamentos conceptuales del aprendizaje por refuerzo y su aplicaci√≥n en Quoridor; el dise√±o experimental, donde se detalla la implementaci√≥n de los agentes y las m√©tricas de evaluaci√≥n; el an√°lisis y discusi√≥n de resultados, que interpreta los hallazgos obtenidos durante las pruebas; y las conclusiones, que sintetizan los aprendizajes y plantean posibles l√≠neas futuras de trabajo.

## Marco Te√≥rico 

Quoridor es un juego de tablero que plantea desaf√≠os estrat√©gicos y t√°cticos. Los jugadores deben avanzar hacia su objetivo mientras colocan barreras para dificultar el progreso del oponente, garantizando siempre un camino accesible hacia la meta. Esta combinaci√≥n de planificaci√≥n ofensiva y defensiva convierte a Quoridor en un entorno ideal para estudiar agentes inteligentes que toman decisiones en tiempo real.

#### Reglas de Quoridor

Quoridor es un juego de mesa en el que el objetivo es llegar a la fila opuesta del tablero antes que tu oponente, evitando que el contrincante llegue a su objetivo colocando obst√°culos. El juego est√° compuesto por el tablero que est√° formado por una cuadr√≠cula de 9x9 casillas, 2 peones(uno para cada jugador) y 20 muros o barreras(10 para cada jugador)

##### Preparaci√≥n:
Cada jugador coloca su pe√≥n en la casilla central de su fila inicial (la fila m√°s cercana a √©l). Cada jugador recibe 10 muros.
![Tablero de quoridor](./quoridor-images/1.png)

##### Turnos:

Los jugadores se turnan para jugar. En su turno, un jugador puede hacer una de las siguientes acciones:

1- Mover su pe√≥n: El pe√≥n puede moverse a una casilla adyacente en direcci√≥n ortogonal (arriba, abajo, izquierda o derecha), siempre que no haya un muro bloqueando el paso.
![Tablero de quoridor](./quoridor-images/2.png)

2- Colocar un muro: Se puede colocar un muro entre dos casillas para bloquear el paso del oponente. Los muros deben colocarse entre dos intersecciones de l√≠neas y no pueden dividir el tablero en dos partes completamente aisladas.

##### Reglas adicionales:

* Si un pe√≥n est√° adyacente al pe√≥n del oponente y no hay un muro entre ellos, puede saltarlo y ocupar su casilla.
![Tablero de quoridor](./quoridor-images/3.png)
* Si hay un muro detr√°s del pe√≥n del oponente, se puede mover en direcci√≥n lateral o diagonal.  
![Tablero de quoridor](./quoridor-images/4.png)
![Tablero de quoridor](./quoridor-images/5.png)
* No se pueden recuperar muros una vez colocados.

El juego termina cuando un jugador alcanza la fila opuesta con su pe√≥n y se le declara ganador.

#### Aprendizaje por Refuerzo

El aprendizaje por refuerzo se basa en la interacci√≥n entre un agente y un entorno. El agente observa el estado del entorno, ejecuta una acci√≥n, recibe una recompensa y transita hacia un nuevo estado. El objetivo del agente es aprender una pol√≠tica que maximice la recompensa acumulada a lo largo del tiempo. La pol√≠tica puede ser una funci√≥n determinista o probabil√≠stica que asocia estados con acciones.

Un concepto central en RL es la funci√≥n de valor, que estima la recompensa esperada para un estado o una combinaci√≥n estado-acci√≥n. En este trabajo, se utiliza el algoritmo Q-learning, que aproxima la funci√≥n de valor √≥ptima (ùëÑ\*) sin requerir un modelo expl√≠cito del entorno. Este algoritmo utiliza una estructura conocida como Q-table para almacenar los valores Q y actualizarlos mediante la f√≥rmula:

    ùëÑ(ùë†,ùëé) ‚Üê ùëÑ(ùë†,ùëé) \+ ùõº \[ùëü \+ ùõæmax\~ùëé‚Ä≤\~ ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤) ‚àí ùëÑ(ùë†,ùëé)\]

Donde:

*  ùë† y ùë†‚Ä≤ son el estado actual y el siguiente.  
*  ùëé y ùëé‚Ä≤ son las acciones actuales y futuras.  
*  ùëü es la recompensa recibida.  
*  ùõº es la tasa de aprendizaje.  
*  ùõæ es el factor de descuento.

#### Importancia del Aprendizaje por Refuerzo en Juegos de Mesa

El aprendizaje por refuerzo (RL) ha sido fundamental en la resoluci√≥n de juegos de mesa cl√°sicos, permitiendo a las m√°quinas aprender estrategias complejas sin conocimiento previo expl√≠cito del dominio. Ejemplos destacados incluyen AlphaGo, que super√≥ a jugadores profesionales de Go utilizando una combinaci√≥n de RL y redes neuronales profundas, y AlphaZero, que demostr√≥ una capacidad sobresaliente en ajedrez, shogi(un juego japon√©s parecido al ajedrez) y GO, partiendo solo de reglas b√°sicas y autoaprendizaje.

La aplicaci√≥n de RL en Quoridor representa un desaf√≠o similar, ya que involucra la planificaci√≥n estrat√©gica y la adaptaci√≥n a las decisiones del oponente. A diferencia de juegos como el ajedrez, donde las piezas tienen reglas de movimiento fijas, Quoridor introduce barreras que pueden modificar el espacio de juego en cada turno, lo que aumenta la complejidad del aprendizaje.

#### Comparaci√≥n con Otros M√©todos

Aunque este trabajo emplea Q-learning, otros enfoques, como el **Monte Carlo Tree Search (MCTS)**, han sido explorados previamente en Quoridor debido a su capacidad para evaluar de forma eficiente m√∫ltiples secuencias de movimientos. Sin embargo, el MCTS es computacionalmente intensivo y requiere un modelo expl√≠cito del juego, lo que lo hace menos adecuado para entrenar agentes adaptativos en un entorno din√°mico. En contraste, Q-learning permite a los agentes aprender directamente de la interacci√≥n con el entorno, adapt√°ndose al comportamiento del oponente y a reglas modificadas.

En problemas de mayor complejidad, como juegos con grandes espacios de estado, se podr√≠an utilizar t√©cnicas m√°s avanzadas, como Deep Q-Networks (DQN). En el caso de Quoridor, el n√∫mero de estados posibles es extremadamente grande debido a la combinaci√≥n de posiciones de los jugadores y las barreras colocadas. Esto hace que una Q-table tradicional sea dif√≠cil de manejar, pues requiere almacenar una cantidad masiva de combinaciones de estados y acciones.Para abordar el problema del espacio de estado demasiado grande, Deep Q-Networks (DQN) reemplaza la Q-table con una red neuronal que aproxima la funci√≥n Q(s,a). En lugar de almacenar valores expl√≠citos en una tabla, la red neuronal aprende a predecir la mejor acci√≥n en funci√≥n de las caracter√≠sticas del estado.

Si bien en este trabajo se ha optado por Q-learning debido a su simplicidad y menor costo computacional, un enfoque basado en DQN podr√≠a ser m√°s adecuado para un agente que juegue Quoridor de manera √≥ptima. Esto se debe a que:

* La representaci√≥n expl√≠cita de todos los estados en una tabla es impracticable en Quoridor debido al gran espacio de b√∫squeda.  
* Una red neuronal podr√≠a aprender patrones generales del juego sin necesidad de almacenar cada estado individualmente.  
* DQN permitir√≠a la generalizaci√≥n en tableros de diferentes tama√±os o contra distintos tipos de oponentes.

#### Relevancia del Aprendizaje por Refuerzo en Quoridor

La naturaleza secuencial y competitiva de Quoridor lo convierte en un entorno ideal para investigar el balance entre exploraci√≥n y explotaci√≥n, el dise√±o de recompensas, y la representaci√≥n eficiente de estados y acciones. Adem√°s, las adaptaciones introducidas en este proyecto, como el sistema de puntuaci√≥n y los tableros de diferentes tama√±os, permiten evaluar c√≥mo la complejidad del entorno afecta el aprendizaje y la estrategia del agente.

## Dise√±o Experimental

Para poner a prueba el algoritmo de Q-learning aplicado al entorno de quoridor se tiene que tener en cuenta por un lado la manera en la que se va a recompensar o penalizar al agente por cada acci√≥n que tome seg√∫n el estado en el que se encuentre, y por el otro lado hacer un proceso se sintonizaci√≥n o tuning de los hiper-par√°metros del algoritmo, es decir, se van a ajustar los distintos par√°metros del algoritmo de q-learning(alpha, gamma, epsilon), como as√≠ tambi√©n considerar que factor de descuento de la tasa de exploraci√≥n(epsilon decay) tras cada episodio para buscar el mejor rendimiento en el problema de Quoridor, ya que al haber un espacio de acciones tan grande, si bien hay que fomentar la exploraci√≥n del agente, a medida que avanza el aprendizaje hay que disminuir paulatinamente este par√°metro para explotar las mejores acciones conocidas. Otro factor que hay que tener en cuenta es el entorno en el que se va a realizar esta sintonizaci√≥n, si bien el tablero original en el que se eval√∫a el agente es de 9x9, se pueden reducir los tiempos de este proceso llev√°ndolo a cabo en el entorno de 5x5 debido al gran tiempo de c√≥mputo que requieren las simulaciones.

Una vez obtenidos los datos de las simulaciones con las distintas combinaciones de par√°metros, se proceder√° a utilizar librer√≠as de python para graficar los resultados y as√≠ poder visualizar de mejor manera la forma en la que var√≠a el desempe√±o del agente con las variantes probadas. Este proceso se repetir√° las veces que sea necesario para poder ajustar lo mejor posible la configuraci√≥n del agente.

### Aplicaci√≥n del Aprendizaje por Refuerzo en Quoridor

En el juego de Quoridor, el entorno est√° representado por el tablero de 9x9 casillas. En este proyecto, se trabajan dos variantes del juego: un tablero est√°ndar de 9x9 casillas y una versi√≥n simplificada de 5x5. El uso de un tablero m√°s peque√±o facilita la exploraci√≥n y entrenamiento inicial del agente basado en Q-learning, permitiendo un an√°lisis comparativo del impacto de la complejidad del entorno en el desempe√±o de los agentes.

Adem√°s, se introducen modificaciones como un sistema de puntuaci√≥n basado en el progreso hacia la meta y la colocaci√≥n de barreras. Estas reglas adaptadas permiten evaluar de forma m√°s granular las estrategias y decisiones tomadas por los agentes.

En este contexto los estados encapsulan informaci√≥n sobre las posiciones de los peones, las barreras colocadas y las barreras restantes de cada jugador. Las acciones disponibles incluyen mover el pe√≥n a una casilla v√°lida (horizontal, vertical o mediante saltos) y colocar barreras en ubicaciones permitidas. El objetivo del agente es llegar a la fila opuesta o maximizar su puntaje siguiendo el esquema adaptado.

La recompensa se dise√±a para reflejar el progreso estrat√©gico, el primer dise√±o que se le da a la pol√≠tica de recompensas del agente es la siguiente:

    1- Movimientos hacia la meta: Otorgan recompensas crecientes (2^n, seg√∫n la fila alcanzada).

    2- Movimientos en sentido opuesto: Se penaliza con 2^n, siguiendo el mismo criterio.

    3- Movimientos laterales: Penaliza de manera progresiva con 2*n¬∞_turno.

    4- Colocaci√≥n de barreras: En un principio se penalizar√° con 10 * tama√±o_del_entorno si la barrera no bloquea la columna por la que avanza el oponente, y recompensar√° con tama√±o_del _entorno¬≤ si lo hace.

Este dise√±o recompensa las acciones que acercan al agente a su objetivo, penaliza movimientos regresivos, aumenta la penalizaci√≥n de movimientos laterales a medida que avanza el juego para evitar que quede encerrado en trampas en forma de U e intenta incentivar el uso de barreras horizontales para evitar que el oponente llegue a la meta. Este dise√±o simple se aplica en un principio por la dificultad que conlleva crear pol√≠ticas ideales que lleven al agente a jugar de manera √≥ptima. Finalmente, si bien cada agente empieza la partida siempre en el mismo lado del tablero, se aleatoriza que pe√≥n empieza jugando de manera aleatoria, teniendo cada agente 50% de probabilidades de comenzar jugando.

#### Elecci√≥n de los Hiperpar√°metros de Q-learning

El entrenamiento requiere ajustes en:

* Tasa de aprendizaje (alpha): Determina cu√°nta informaci√≥n nueva se incorpora en cada iteraci√≥n.  
    
* Factor de descuento (gamma): Controla cu√°nta importancia se da a las recompensas futuras.  
    
* Epsilon (exploraci√≥n vs. explotaci√≥n): Define la probabilidad de elegir una acci√≥n aleatoria en lugar de la mejor conocida.  
    
* Epsilon decay: Reduce gradualmente el epsilon a medida que el agente aprende.  
    
* Episodios: Una mayor cantidad de episodios le permitir√° al agente explorar el espacio de acciones para encontrar aquellas que lo acercan m√°s a su objetivo.


#### Exploraci√≥n vs. Explotaci√≥n en el Aprendizaje por Refuerzo

Uno de los desaf√≠os fundamentales en el aprendizaje por refuerzo es el equilibrio entre exploraci√≥n y explotaci√≥n. Este dilema surge porque el agente debe decidir constantemente entre:

**Explotaci√≥n**: Elegir la mejor acci√≥n conocida hasta el momento para maximizar la recompensa inmediata.  
**Exploraci√≥n**: Probar nuevas acciones para descubrir estrategias potencialmente mejores a largo plazo.

En el contexto de Quoridor, la explotaci√≥n significar√≠a que el agente selecciona siempre el movimiento con la mejor recompensa esperada seg√∫n su Q-table. Sin embargo, si el agente explota demasiado temprano, puede quedar atrapado en estrategias sub√≥ptimas sin descubrir mejores caminos para ganar la partida.

Por otro lado, la exploraci√≥n implica tomar decisiones que pueden parecer inicialmente malas (como colocar barreras en posiciones poco intuitivas) con la esperanza de descubrir patrones estrat√©gicos m√°s efectivos a largo plazo. Un agente que explora demasiado puede desperdiciar movimientos y aprender m√°s lentamente, pero tambi√©n puede encontrar soluciones m√°s √≥ptimas con el tiempo.

##### M√©todos de Exploraci√≥n en Q-learning

Para gestionar este equilibrio, existen diversas estrategias de exploraci√≥n:

**Œµ-greedy**: Se elige la mejor acci√≥n conocida con probabilidad 1‚àíŒµ, y una acci√≥n aleatoria con probabilidad Œµ disminuye progresivamente para explorar m√°s al inicio y explotar en etapas avanzadas.

**Softmax**: Las acciones se eligen de forma probabil√≠stica seg√∫n sus valores Q(s,a), permitiendo que incluso las opciones sub√≥ptimas sean seleccionadas ocasionalmente.

**M√©todos basados en incertidumbre (UCB, Bayesian Methods)**: Favorecen acciones menos exploradas, enfoc√°ndose en reducir la incertidumbre en lugar de maximizar recompensas inmediatas.

##### Justificaci√≥n del Uso de Œµ-greedy en Quoridor

La elecci√≥n de Œµ-greedy para este proyecto se basa en los siguientes factores:

1. Facilidad de implementaci√≥n: Es un enfoque simple y efectivo que no requiere c√°lculos adicionales de probabilidades o modelos de incertidumbre.  
2. Adecuaci√≥n a un espacio de acciones discreto: En Quoridor, las acciones (moverse o colocar barreras) son discretas y finitas, lo que hace que este m√©todo sea eficiente.  
3. Balance din√°mico entre exploraci√≥n y explotaci√≥n: Al decrecer Œµ con el tiempo, el agente puede explorar al inicio y explotar estrategias aprendidas en etapas posteriores del entrenamiento.  
4. Evita el estancamiento en estrategias locales sub√≥ptimas: La introducci√≥n de exploraci√≥n aleatoria permite que el agente descubra mejores estrategias en tableros con configuraciones variadas.

Dado que el objetivo es que el agente aprenda estrategias ganadoras sin quedar atrapado en soluciones mediocres, Œµ-greedy proporciona una forma pr√°ctica de permitir exploraci√≥n en las primeras etapas del aprendizaje y favorecer la explotaci√≥n conforme el agente se vuelve m√°s experto en el juego.

#### Desaf√≠os de Convergencia de Q-learning en Espacios de Estado Grandes

Uno de los principales desaf√≠os en el uso de Q-learning es garantizar su convergencia cuando se enfrenta a espacios de estados grandes y complejos, como el tablero de Quoridor. A medida que el n√∫mero de estados crece exponencialmente con el tama√±o del tablero y la cantidad de barreras colocadas, la tabla Q(s,a) se vuelve dif√≠cil de almacenar y actualizar eficientemente. Esto no solo ralentiza el aprendizaje, sino que tambi√©n incrementa el riesgo de que el agente no logre explorar suficientemente el espacio de soluciones, afectando su capacidad para encontrar estrategias √≥ptimas, por lo que nos encontramos con los siguientes desaf√≠os:

1\. Dimensionalidad del Espacio de Estados

En Quoridor, un estado est√° definido por:

* La posici√≥n de ambos jugadores.  
* La cantidad y ubicaci√≥n de las barreras en el tablero.

Esto genera un n√∫mero exponencialmente creciente de posibles configuraciones. En problemas con espacios de estado enormes, almacenar y actualizar una tabla Q(s,a) expl√≠cita se vuelve inviable tanto en t√©rminos de memoria como de tiempo de c√≥mputo.

2\. Convergencia Lenta

El aprendizaje en Q-learning requiere visitar m√∫ltiples veces cada estado y acci√≥n para estimar correctamente sus valores Q(s,a). Sin embargo, en espacios de estado grandes:

* Muchos estados pueden no visitarse con suficiente frecuencia, retrasando la convergencia.  
* Peque√±os errores en la actualizaci√≥n de Q pueden propagarse y afectar decisiones futuras.  
* La exploraci√≥n puede ser ineficiente si no se aplican estrategias bien dise√±adas, como epsilon decay.

3\. Problema de Generalizaci√≥n

Q-learning est√°ndar no generaliza entre estados similares. Cada estado se trata de manera independiente, lo que significa que incluso si dos estados son casi id√©nticos, sus valores  Q(s,a) se aprenden por separado. Por ejemplo, si el agente ha aprendido que bloquear el camino del oponente en una posici√≥n espec√≠fica es √∫til, este conocimiento no se transfiere autom√°ticamente a otras configuraciones similares del tablero.

En este proyecto, Q-learning ha sido implementado de manera tradicional debido a su simplicidad, pero los desaf√≠os mencionados han limitado su capacidad para encontrar estrategias √≥ptimas en todos los escenarios.

#### El Agente Determinista

En contraste con el agente basado en Q-learning, el agente determinista sigue un conjunto de reglas predefinidas para tomar decisiones. Estas reglas incluyen:

* Avanzar hacia la meta siempre que sea posible.   
* Si se encuentra con una barrera, bloquear al oponente mediante la colocaci√≥n de barreras en forma de "U".  
* Si no tiene barreras o el oponente ya est√° ‚Äúatrapado‚Äù, realizar√° movimientos laterales hacia un lado aleatorio para intentar esquivar el obst√°culo.  
* Si ninguna de las acciones anteriores son posibles, entonces retroceder√° para evadir los obst√°culos. 

Este enfoque ofrece un comportamiento predecible, y si bien ser√≠a posible dise√±ar un agente determinista que tenga un mejor desempe√±o que este, la simplicidad de su implementaci√≥n y secuencia de acciones para desenvolverse en el entorno sirve como referencia para evaluar el desempe√±o del agente adaptativo.

### Ajuste de Hiperpar√°metros para el Aprendizaje del Agente

Para buscar los valores √≥ptimos a utilizar, se va a proceder a hacer un barrido de par√°metros que permitan un balance entre la exploraci√≥n y explotaci√≥n de las secuencias de acciones posibles y as√≠ evitar caer en estados sub√≥ptimos. Para esto se van a tomar las siguientes consideraciones:

* Tomar valores peque√±os de alpha, entre 0,01 y 0,2 para no darle tanto peso a las primeras exploraciones del agente, y de esta manera mejorar su desempe√±o mientras m√°s episodios de entrenamiento realice.  
* Se va a tomar un solo valor de gamma relativamente alto(de 0,9), ya que lo que se busca es que al momento de elegir la acci√≥n, los estados futuros tengan un peso significativo, adem√°s al tomar un s√≥lo valor reduce la cantidad de combinaciones para las configuraciones de par√°metros posibles lo que a su vez reduce el tiempo de c√≥mputo.  
* Como al principio del entrenamiento, el agente no tiene ninguna informaci√≥n sobre el entorno, se va a utilizar un valor de epsilon de 1,0, para que comience explorando la mayor cantidad de estados posibles antes de empezar a explotar las mejores acciones conocidas.  
* A su vez se van a probar dos valores para el factor de descuento de epsilon(epsilon decay), uno de 0,99 y otro de 0,999. Estos valores hacen que al agente le lleve m√°s episodios de entrenamiento reducir su tasa de exploraci√≥n, permitiendo expandir su conocimiento del entorno.  
* Adem√°s teniendo en cuenta los valores de epsilon y epsilon decay, se calcul√≥ cu√°ntos episodios se necesitan para que el valor de epsilon sea pr√°cticamente nulo, comenzando con un epsilon de 1,0 en ambos casos, para un epsilon decay de 0,99 tomar√≠a solo 230 episodios para que la tasa de exploraci√≥n alcance el valor de aproximadamente 0,1, mientras que para el epsilon decay de 0,999 se necesitan 2300 episodios para alcanzar un valor similar, por lo que se limitar√° el epsilon m√≠nimo a 0,1 para que con ambas configuraciones se mantenga una probabilidad de exploraci√≥n de al menos el 10%.  
* Tambi√©n se recopilaran los datos de estas configuraciones para distinta cantidad de episodios para luego analizar el aprendizaje del agente. Los valores que se utilizar√°n ser√°n de 250, 500, 1000 y 2500\.

C√≥mo se mencion√≥ anteriormente, estas pruebas se realizar√°n primero en el entorno reducido de 5x5, ya que son partidas m√°s cortas, con un menor espacio de acciones y en consecuencia menor tiempo de c√≥mputo, y para evitar que las partidas se extiendan demasiado por posibles complicaciones de los agentes para superar obst√°culos, se limitar√° la cantidad m√°xima de turnos en 100 para este entorno, y luego seg√∫n los resultados obtenidos se analizar√° qu√© valores utilizar en el entorno original. Finalmente se correr√°n 100 pruebas de cada configuraci√≥n despu√©s del entrenamiento para tomar m√©tricas m√°s fiables y evitar resultados estoc√°sticos.

### Evaluaci√≥n del Agente en el Entorno Original

Tras analizar los resultados obtenidos en el entorno reducido de 5x5, se decidi√≥ evaluar el desempe√±o del agente en el entorno original de Quoridor (9x9) utilizando las configuraciones m√°s prometedoras. Dado que la expansi√≥n del espacio de estados y de acciones introduce una mayor complejidad en el aprendizaje, se ajustaron los hiperpar√°metros para favorecer una exploraci√≥n inicial m√°s amplia y permitir una adaptaci√≥n progresiva.

#### Consideraciones para el Entorno 9x9

Dado que en el entorno de 5x5 el agente mostr√≥ mejor desempe√±o cuando la tasa de exploraci√≥n disminu√≠a de manera m√°s progresiva, se prioriz√≥ el uso de **Epsilon Decay \= 0.999** en el entorno 9x9. Este ajuste busca evitar que el agente se estanque prematuramente en estrategias sub√≥ptimas y pueda seguir explorando de manera efectiva en un espacio de acci√≥n m√°s amplio.

Asimismo, los mejores resultados en tasa de victorias en el entorno 5x5 se obtuvieron con Alpha = 0.2 y una cantidad intermedia de episodios. Sin embargo, debido al aumento de complejidad en 9x9, se evaluar√°n tanto valores de Alpha = 0.1 como Alpha \= 0.2 para determinar cu√°l permite una mejor convergencia en el aprendizaje.

Otro factor clave observado en 5x5 fue la relaci√≥n entre episodios de entrenamiento y rendimiento. Aunque en algunos casos 500 episodios fueron suficientes para lograr una tasa de victorias alta, en configuraciones con mayor Epsilon Decay se requiri√≥ un mayor n√∫mero de episodios para consolidar estrategias. Por lo tanto, en 9x9 se probar√°n configuraciones con 1000, 2500 y 5000 episodios para evaluar c√≥mo la cantidad de entrenamiento impacta en el rendimiento.

## An√°lisis de Resultados

El siguiente an√°lisis tiene como objetivo evaluar el desempe√±o del agente Q-learning primeramente en el entorno de Quoridor de 5x5 bajo distintas configuraciones de hiperpar√°metros. Para ello, se han realizado simulaciones variando valores clave como la tasa de aprendizaje (Œ±), el factor de descuento (Œ≥), la exploraci√≥n inicial (Œµ), la tasa de decaimiento de Œµ y la cantidad de episodios de entrenamiento.

### Resultados del Barrido de Par√°metros en el Entorno Reducido

| Alpha | Gamma | Epsilon | Epsilon Decay | Episodes | Tests Qty | Win Rate | Avg Score | Avg Barriers Placed |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 0.1 | 0.9 | 1.0 | 0.99 | 250 | 100 | 2.0 | \-31.53 | 3.0 |
| 0.1 | 0.9 | 1.0 | 0.99 | 500 | 100 | 12.0 | 11.49 | 2.53 |
| 0.1 | 0.9 | 1.0 | 0.99 | 1000 | 100 | 1.0 | 34.64 | 2.54 |
| 0.1 | 0.9 | 1.0 | 0.99 | 2500 | 100 | 0.0 | 37.49 | 2.22 |
| 0.1 | 0.9 | 1.0 | 0.999 | 250 | 100 | 7.0 | \-136.49 | 2.93 |
| 0.1 | 0.9 | 1.0 | 0.999 | 500 | 100 | 3.0 | \-64.12 | 2.77 |
| 0.1 | 0.9 | 1.0 | 0.999 | 1000 | 100 | 1.0 | \-41.14 | 2.93 |
| 0.1 | 0.9 | 1.0 | 0.999 | 2500 | 100 | 0.0 | 43.52 | 2.19 |
| 0.2 | 0.9 | 1.0 | 0.99 | 250 | 100 | 1.0 | \-44.44 | 2.96 |
| 0.2 | 0.9 | 1.0 | 0.99 | 500 | 100 | 39.0 | 8.78 | 2.96 |
| 0.2 | 0.9 | 1.0 | 0.99 | 1000 | 100 | 3.0 | \-60.37 | 2.99 |
| 0.2 | 0.9 | 1.0 | 0.99 | 2500 | 100 | 35.0 | 20.33 | 3.0 |
| 0.2 | 0.9 | 1.0 | 0.999 | 250 | 100 | 3.0 | \-95.27 | 2.87 |
| 0.2 | 0.9 | 1.0 | 0.999 | 500 | 100 | 4.0 | \-60.95 | 2.68 |
| 0.2 | 0.9 | 1.0 | 0.999 | 1000 | 100 | 9.0 | \-70.45 | 2.97 |
| 0.2 | 0.9 | 1.0 | 0.999 | 2500 | 100 | 1.0 | 43.3 | 2.77 |

![Variaci√≥n de Score alpha=0.1 5x5](./code/results-images/tuning_results_1.png)
***Imagen 1: Variaci√≥n del puntaje promedio con alpha = 0.1 para distintos valores de epsilon decay en el entorno reducido(5x5).***

![Variaci√≥n de Score alpha=0.2 5x5](./code/results-images/tuning_results_2.png)
***Imagen 2: Variaci√≥n del puntaje promedio con alpha = 0.2 para distintos valores de epsilon decay en el entorno reducido(5x5).***

#### Exploraci√≥n vs. Explotaci√≥n

Cuando Epsilon Decay es 0.99, el agente sigue explorando m√°s durante el entrenamiento, sin embargo, no siempre conduce a un mejor desempe√±o final. En cambio, cuando Epsilon Decay es 0.999, la exploraci√≥n disminuye m√°s lentamente, permitiendo consolidar estrategias aprendidas.  
En general, un mayor Epsilon Decay tiende a mostrar menor variabilidad en el rendimiento, lo que podr√≠a indicar un equilibrio m√°s estable entre exploraci√≥n y explotaci√≥n.


#### Impacto de los Episodios en el Rendimiento

En configuraciones con 250 episodios, el agente muestra una baja tasa de victorias (Win Rate ‚â§ 7%), indicando que a√∫n no ha aprendido una estrategia efectiva.  
A medida que aumentan los episodios (500, 1000, 2500), el desempe√±o var√≠a, mostrando en algunos casos mejoras en el puntaje promedio y el uso de barreras, pero sin una tendencia clara de optimizaci√≥n.  
En ciertos casos (por ejemplo, Epsilon Decay \= 0.999, Episodes \= 2500), se observa que el agente logra puntajes positivos, sugiriendo que un mayor entrenamiento puede mejorar su rendimiento, sin embargo en otros casos (por ejemplo, Epsilon Decay \= 0.99 y Episodes \= 500\) consigue algunos de los mayores porcentajes de Win Rate(12% con Alpha \= 0.1 y 39% con Alpha \= 0.2) por lo que pareciera que mientras m√°s alta es la tasa de aprendizaje en este entorno reducido y mayor el decaimiento de la tasa de exploraci√≥n, con menor cantidad de episodios logra mayor tasa de victorias.

#### Comparaci√≥n entre Configuraciones

* Mejor resultado en tasa de victorias:  
1. (Œ±=0.2, Œ≥=0.9, Œµ=1.0, Decay=0.99, Episodes=500) ‚Üí 39% Win Rate, con un puntaje positivo (8.78).  
2. (Œ±=0.2, Œ≥=0.9, Œµ=1.0, Decay=0.99, Episodes=2500) ‚Üí 35% Win Rate y 20.33 de puntaje.

En ambas configuraciones se alcanz√≥ la cota inferior en la tasa de exploraci√≥n, pero dado que el espacio de acciones en el entorno original es mayor, probablemente se necesite que la tasa de exploraci√≥n disminuya m√°s lento.

* Peores desempe√±os:

* Varias configuraciones muestran Win Rate \= 0%, lo que sugiere que el agente no logr√≥ encontrar estrategias efectivas en esos casos, aunque no habr√≠a que descartar aquellas en las que la tasa de exploraci√≥n ya era baja y se consiguieron puntajes positivos(por ejemplo, Œ±=0.1, Decay=0.99, Episodes=2500).  
* En configuraciones como (Œ±=0.1, Œ≥=0.9, Œµ=1.0, Decay=0.999, Episodes=1000), el puntaje promedio es negativo (-70.45), lo que indica que el agente sigue cometiendo errores importantes. Probablemente debido a que la tasa de exploraci√≥n en esos casos es de aproximadamente del 37%. Por lo que para configuraciones con Decay=0.999 van a requerir mayor cantidad de episodios.

### Resultados del barrido de par√°metros en el entorno original

Tras evaluar el desempe√±o del agente en el entorno 9x9 con Epsilon Decay \= 0.999 y distintos valores de Alpha (0.1 y 0.2), se observaron patrones clave en la evoluci√≥n del aprendizaje y el rendimiento del agente.

| Alpha | Gamma | Epsilon | Epsilon\_Decay | Episodes | Tests\_qty | Win\_Rate | Avg\_Score | Avg\_Barriers\_Placed |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 0.1 | 0.9 | 1.0 | 0.999 | 1000 | 100 | 0.0 | \-271.7 | 7.74 |
| 0.1 | 0.9 | 1.0 | 0.999 | 2500 | 100 | 0.0 | \-63.28 | 8.89 |
| 0.1 | 0.9 | 1.0 | 0.999 | 5000 | 100 | 1.0 | \-62.12 | 9.46 |
| 0.2 | 0.9 | 1.0 | 0.999 | 1000 | 100 | 1.0 | \-525.7 | 8.99 |
| 0.2 | 0.9 | 1.0 | 0.999 | 2500 | 100 | 0.0 | \-27.03 | 9.3 |
| 0.2 | 0.9 | 1.0 | 0.999 | 5000 | 100 | 1.0 | 84.45 | 8.35 |

![Evoluci√≥n de la recompensa con el entrenamiento](./code/results-images/avg_score_9x9.png)
***Imagen 3: Variaci√≥n del puntaje promedio para distintos alphas seg√∫n la cantidad de episodios de entrenamiento en el entorno original(9x9).***

#### Impacto de la cantidad de episodios

El n√∫mero de episodios de entrenamiento tuvo una influencia significativa en el rendimiento del agente dentro del entorno 9x9. En particular, se observ√≥ que un menor n√∫mero de episodios resultaba en un desempe√±o deficiente, mientras que un aumento progresivo en la cantidad de iteraciones permiti√≥ una mejora notable en la eficacia del agente.

En las pruebas realizadas con 1000 episodios, el agente present√≥ un rendimiento insatisfactorio en todas las configuraciones evaluadas. Para Alpha \= 0.1, el puntaje promedio (Avg\_Score) alcanzado fue de \-271.7, mientras que con Alpha \= 0.2 el desempe√±o fue a√∫n peor, con un Avg\_Score de \-525.7. Adem√°s, en ambos casos, la tasa de victorias fue pr√°cticamente nula, lo que indica que el agente no logr√≥ desarrollar estrategias efectivas en esta fase de entrenamiento.

Al incrementar la cantidad de episodios a 2500, se observ√≥ una mejora significativa en los resultados. Para Alpha \= 0.1, el Avg\_Score aument√≥ a \-63.28, aunque la tasa de victorias se mantuvo en 0%. Por otro lado, con Alpha \= 0.2, el agente evidenci√≥ una mejor adaptaci√≥n al entorno, alcanzando un Avg\_Score de \-27.03, lo que sugiere que comenz√≥ a desarrollar estrategias m√°s eficientes.

![Puntaje promedio en 9x9 con alpha=0.1](./code/results-images/avg_score_vs_episodes_9x9.png)
***Imagen 4: Evoluci√≥n del puntaje promedio cada 50 episodios de entrenamiento con alpha = 0.2 en el entorno original(9x9).***

El mejor rendimiento registrado se obtuvo con 5000 episodios, donde el agente mostr√≥ un progreso m√°s s√≥lido. En el caso de Alpha \= 0.1, el Avg\_Score se estabiliz√≥ en \-62.12, acompa√±ado de una m√≠nima mejora en la tasa de victorias, que alcanz√≥ el 1%. Con Alpha \= 0.2, el desempe√±o mejor√≥ a√∫n m√°s, logrando un Avg\_Score positivo de 84.45 y manteniendo una tasa de victorias del 1%.

Estos resultados demuestran que la cantidad de episodios de entrenamiento es un factor cr√≠tico en el entorno 9x9. A diferencia de lo observado en el entorno 5x5, donde algunas configuraciones lograban una tasa de victorias considerable con menos de 1000 episodios, en un espacio de mayor complejidad como 9x9 es necesario un entrenamiento prolongado para que el agente pueda refinar su estrategia de manera efectiva.

#### Comparaci√≥n entre Alpha \= 0.1 y Alpha \= 0.2

El impacto de la tasa de aprendizaje (Alpha) en el desempe√±o del agente fue evidente a lo largo del entrenamiento, reflej√°ndose en la evoluci√≥n del Avg\_Score y en la capacidad del agente para mejorar su estrategia con el tiempo.

En el caso de Alpha \= 0.1, se observ√≥ una mejora progresiva en el Avg\_Score a medida que aumentaba la cantidad de episodios. Sin embargo, este progreso fue limitado, ya que, si bien los puntajes negativos se redujeron con m√°s entrenamiento, la tasa de victorias permaneci√≥ pr√°cticamente nula. Esto sugiere que el agente tuvo dificultades para consolidar estrategias ganadoras, posiblemente debido a una convergencia m√°s lenta o a la persistencia de decisiones sub√≥ptimas a lo largo del entrenamiento.

Por otro lado, Alpha \= 0.2 permiti√≥ un aprendizaje m√°s acelerado, obteniendo mejores resultados en un menor n√∫mero de episodios. A pesar de que su rendimiento inicial en 1000 episodios fue inferior al de Alpha \= 0.1, con un Avg\_Score de \-525.7, su progreso a lo largo del entrenamiento fue m√°s notable. En 5000 episodios, el agente logr√≥ un Avg\_Score positivo de 84.45, lo que indica que no solo explor√≥ de manera m√°s eficiente el entorno, sino que tambi√©n pudo explotar estrategias m√°s exitosas en fases avanzadas del entrenamiento. Sin embargo, es importante evaluar hasta qu√© punto un Alpha elevado sigue siendo ventajoso, ya que un valor excesivamente alto podr√≠a impedir la estabilidad en el aprendizaje.

## Conclusiones

Si bien el agente ha mostrado una mejora progresiva en las puntuaciones obtenidas a medida que se incrementa la cantidad de episodios de entrenamiento, este avance no se traduce en un aumento significativo en la tasa de victorias. Esto sugiere que, aunque el agente est√° aprendiendo estrategias m√°s eficientes para moverse en el entorno 9x9, estas no necesariamente conducen a una mayor probabilidad de ganar.

El valor de Alpha = 0.2 ha demostrado ser m√°s efectivo que Alpha = 0.1, permitiendo una mejor adaptaci√≥n y alcanzando valores positivos de Avg_Score en 5000 episodios. Sin embargo, la baja tasa de victorias indica que la funci√≥n de recompensa puede estar incentivando estrategias que optimizan la puntuaci√≥n sin priorizar la victoria.

Otro aspecto relevante es el uso de barreras, que var√≠a a lo largo del entrenamiento y sugiere que el agente modifica su estrategia con el tiempo. Esto plantea la posibilidad de que, con m√°s episodios o un refinamiento en la pol√≠tica de exploraci√≥n, el agente pueda desarrollar t√°cticas a√∫n m√°s sofisticadas.

Dado que el dise√±o actual de la funci√≥n de recompensa podr√≠a estar influyendo en estos resultados, futuros experimentos podr√≠an centrarse en ajustar estos par√°metros. Modificaciones en la forma en que se recompensa el uso de barreras o el acercamiento a la victoria podr√≠an ayudar a alinear mejor el aprendizaje del agente con el objetivo final de ganar partidas.

## Bibliograf√≠a

\[1\] Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Pearson.

\[2\] Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.

\[3\] Lapan, M. (2018). Deep Reinforcement Learning Hands-On. Packt Publishing.

\[4\] Massagu√© Respall, V., Brown, J., & Aslam, H. (2018). Monte Carlo Tree Search for Quoridor. ResearchGate.

\[5\] Wang, H., Emmerich, M., & Plaat, A. (2019). Assessing the Potential of Classical Q-learning in General Game Playing. In Proceedings of the International Conference on Agents and Artificial Intelligence. ResearchGate.

\[6\] DataCamp. (n.d.). Introduction to Q-Learning: Beginner Tutorial. Retrieved from [https://www.datacamp.com/es/tutorial/introduction-q-learning-beginner-tutorial](https://www.datacamp.com/es/tutorial/introduction-q-learning-beginner-tutorial)

\[7\] Gymnasium. (n.d.). Train an Agent using Gymnasium. Retrieved from https://gymnasium.farama.org/